{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcb50f0d",
   "metadata": {},
   "source": [
    "# Stroke Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe92c44",
   "metadata": {},
   "source": [
    "# Goals\n",
    "* Discover the drivers that increase the risk factor of stroke\n",
    "* Use drivers to develop a machine learning model to predict weather or not a patient has had a stroke.\n",
    "* Use findings to see what preventitave measures if any can be taken to decrease the risk of stroke"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a6c7c2",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "284617e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "\n",
    "import wrangle as w\n",
    "import explore as e\n",
    "import model as m\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b4faa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree and Model Evaluation Imports\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c8a038",
   "metadata": {},
   "source": [
    "# Acquire\n",
    "* Data acquired from kaggle on 12-13-2022\n",
    "    https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset?select=healthcare-dataset-stroke-data.csv\n",
    "* Data contains 5110 rows and 12 columns\n",
    "* Each row represents a patient and information about the patient\n",
    "* Each column representa a features related to the patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0abbfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# acquire data from csv file\n",
    "df= w.get_stroke_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd8e38f",
   "metadata": {},
   "source": [
    "# Prepare\n",
    "prepare actions:\n",
    "  * removed columns with unnecessary information\n",
    "  * renamed columns and values\n",
    "  * 211 null values where found and will be imputed after split\n",
    "  * Outliers have not been removed\n",
    "  * Encoded categorical variables\n",
    "  * Split data into train, validate and test:\n",
    "        (Approximately: train 56%, validate 24%, test 20%)\n",
    "\n",
    "  * impute 211 null values under BMI column with median due to the right skewness of feature \n",
    "  \n",
    "  * Continuous variables will be scaled before modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb540d5f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# prepare data\n",
    "df = w.data_prep(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51db4bb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# split data and impute bmi by mean\n",
    "train, validate, test = w.split_and_impute_data(df, 'stroke')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abcf41a",
   "metadata": {},
   "source": [
    "# Data Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f8da65",
   "metadata": {},
   "source": [
    "| Feature    | Description    | \n",
    "| :------------- | -------------: | \n",
    "|      id     |   unique identifier      | \n",
    "|   gender       |    Gender of patient Male or Female or Other     | \n",
    "| age|     age of the patient     | \n",
    "|      hypertension    |     If a patient has hypertension  (1 = yes, 0 = no)  | \n",
    "|      hear_disease     |     If a patient has any heart diseases  (1 = yes, 0 = no)  | \n",
    "|       ever_married   |   If a patient has ever been married (1 = yes, 0 = no)      | \n",
    "|    work_type       |    The work type of patient. *Children are under children category     | \n",
    "|  residence_type        |     If a patient residence is  rural of urban   | \n",
    "|   avg_glucose_level        |    average glucose level in blood     | \n",
    "|     bmi     |   body mass index of patient      | \n",
    "|   smoking_status       |    smoking status of patient * Unknown represent the information was unavailable     | \n",
    "|          |         | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2591cc",
   "metadata": {},
   "source": [
    "# Explore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18f1eda",
   "metadata": {},
   "source": [
    "# 1. What is the percent of patients who have a stroke?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db51dca1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# obtain visualization for percent of patients who have suffer a stroke\n",
    "e.viz_stroke_percentage(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63ca489",
   "metadata": {},
   "source": [
    "***About 5 % of our patients have suffer a stroke***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23db6d6c",
   "metadata": {},
   "source": [
    "# 2. Does the presense of hypertension increase the risk of stroke?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb4d4c98",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# obtain visualization for hypertension vs stroke\n",
    "e.viz_hypertension_vs_stroke(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd7a1d1",
   "metadata": {},
   "source": [
    "***It appears that patients with hypertension tend to have a higher stroke rate than patients without hypertension***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7184d3",
   "metadata": {},
   "source": [
    "### I will now conduct a chi-square test to determine if there is an association between hypertension  and stroke.\n",
    "\n",
    "* The confidence interval is 95%\n",
    "* Alpha is set to 0.05\n",
    "\n",
    "$H_0$ : Hypertension is **independent**   of stroke.\n",
    "\n",
    "$H_a$: Hypertension is **dependent** of stroke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b45a6f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Chi-Square:24.45069324334392\n",
      " p-value:7.623607564219725e-07\n"
     ]
    }
   ],
   "source": [
    "# obtain chi square test\n",
    "e.chi_square_test(train,'stroke', 'hypertension')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f609740",
   "metadata": {},
   "source": [
    "Since the p-value is less than alpha we reject the null hypothesis. There is evidence to support that hypertention has an association with stroke. I believe that hypertention is a driver of stroke. Adding an encoded version of this feature to the model will likely increase the mode's accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a945c9a3",
   "metadata": {},
   "source": [
    "# 3. Are patients with a heart condition more at risk of stroke than patients with hypertension?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "001b52b4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# obtain visualization for heart disease populatin vs stroke and hypertension population vs stroke\n",
    "e. viz_heart_hyper_stroke(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ec38c1",
   "metadata": {},
   "source": [
    "***It appears that patients with a heart condition tend to have a higher stroke rate than patients with hypertension***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d3e62a",
   "metadata": {},
   "source": [
    "### I will now conduct a chi-square test to determine if there is an association between heart condition  and stroke.\n",
    "\n",
    "* The confidence interval is 95%\n",
    "* Alpha is set to 0.05\n",
    "\n",
    "$H_0$: Patients with a heart condition is **independent**   of stroke.\n",
    "\n",
    "$H_a$: Patients with a heart condition **dependent** of stroke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8233ae22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Chi-Square:36.73001998146243\n",
      " p-value:1.356740136221253e-09\n"
     ]
    }
   ],
   "source": [
    "# obtain chi square test\n",
    "e.chi_square_test(train,'stroke', 'heart_disease')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1de722",
   "metadata": {},
   "source": [
    "Since the p-value is less than alpha we reject the null hypothesis. There is evidence to support that patients with heart disease has an association with stroke. I believe that heart disease is a driver of stroke. Adding an encoded version of this feature to the model will likely increase the mode's accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbb5645",
   "metadata": {},
   "source": [
    "# 4. Controling for gender of a patient, does heart disease increases risk of stroke?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "344cb661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create subgroups for male and female\n",
    "male_subset = train[train.gender_Male==1]\n",
    "female_subset = train[train.gender_Male==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44b9fe99",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# obtain visualization controling for gender heart disease vs stroke\n",
    "e.viz_gender_heart_stroke(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efcfd71",
   "metadata": {},
   "source": [
    "***It appears that males with hypertension have an above average stroke rate.\n",
    "Also males with hypertensio have a higher stroke rate than females***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e5f58c",
   "metadata": {},
   "source": [
    "### I will now conduct a chi-square test  is the male population with heart disease associated with stroke.\n",
    "\n",
    "* The confidence interval is 95%\n",
    "* Alpha is set to 0.05\n",
    "\n",
    "$H_0$ : Males with hypertension is **independent**   of stroke.\n",
    "\n",
    "$H_a$: Males with hypertension is **dependent** of stroke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcb810a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Chi-Square:29.08260992156263\n",
      " p-value:6.935676062013279e-08\n"
     ]
    }
   ],
   "source": [
    "e.chi_square_test(male_subset,'stroke', 'heart_disease')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d41fc2",
   "metadata": {},
   "source": [
    "Since the p-value is less than alpha we reject the null Hypothesis. There is evidence to support that males a heart disease has an association with stroke.  Adding an encoded version of this feature to the model will likely increase the mode's accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500d8e71",
   "metadata": {},
   "source": [
    "### I will now conduct a chi-square test  is the female population with heart disease associated with stroke.\n",
    "* The confidence interval is 95%\n",
    "* Alpha is set to 0.05\n",
    "\n",
    "$H_0$ : Females with hypertension is **independent**   of stroke.\n",
    "\n",
    "$H_a$: Females with hypertension is **dependent** of stroke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6fb7d95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Chi-Square:6.8306066361801525\n",
      " p-value:0.008960882580754448\n"
     ]
    }
   ],
   "source": [
    "e.chi_square_test(female_subset,'stroke', 'heart_disease')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2d6d71",
   "metadata": {},
   "source": [
    "Since the p-value is less than alpha we reject the null Hypothesis. There is evidence to support that females with a heart diseas has an association with stroke. Adding an encoded version of this feature to the model will likely increase the mode's accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd9a3ed",
   "metadata": {},
   "source": [
    "# 5. Is age a driver of stroke?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b95281cd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# obtain graph age vs stroke\n",
    "e.viz_age_vs_stroke(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2f1fce",
   "metadata": {},
   "source": [
    "***It seems that with as our patients age increases so does the instances of occurance of a stroke.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5022836e",
   "metadata": {},
   "source": [
    "### I will conduct a t-test to determine if the population who has suffer a stroke has a higher mean age thatn the population who has not had a stoke occurance.\n",
    "* The confidence interval is 95%\n",
    "* Alpha is set to 0.05\n",
    "* This will be a Independent t-test\n",
    "\n",
    "$H_0$ : The mean age of patients who have suffer a stroke <= to the mean age of patients who have not suffer a stroke\n",
    "\n",
    "$H_a$: The mean age of patients who have suffer a stroke > to the mean age of patients who have not suffer a stroke\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6263cdaf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t-stat 20.108050826301564\n",
      "p-value 5.1422688960466496e-48\n"
     ]
    }
   ],
   "source": [
    "# obtain ttest for age vs stroke\n",
    "e.ttest(train,'stroke','age')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca72930f",
   "metadata": {},
   "source": [
    "Since the p-value/2 is less than alpha we reject the null Hypothesis. There is evidence to support that patients who suffer a stroke  on average are older than patients who have not suffer a stroke. Based on this statistical finding I believe that age is a driver of stroke. Adding an scaled version of this feature to the model will likely increase the model ability to predict home value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd68906",
   "metadata": {},
   "source": [
    "# 6. Do patients who have ever been married suffer more strokes than patients that have not been married"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85ce0b4d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualization for patients who have been married vs stroke\n",
    "e.viz_marriage_vs_stroke(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8ddfed",
   "metadata": {},
   "source": [
    "***There seems to be a slight increase in stroke for those patients who have been married.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0ebb64",
   "metadata": {},
   "source": [
    "### I will now conduct a chi-square test  if patients who have been married is associated with stroke.\n",
    "\n",
    "* The confidence interval is 95%\n",
    "* Alpha is set to 0.05\n",
    "\n",
    "$H_0$ : Patients who have been married are **independent**   of stroke.\n",
    "\n",
    "$H_a$: Patients who have been married are not **dependent** of stroke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "acb1e4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Chi-Square:33.01784652416314\n",
      " p-value:9.131679279313294e-09\n"
     ]
    }
   ],
   "source": [
    "e.chi_square_test(train, 'stroke','ever_married_Yes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e7772f",
   "metadata": {},
   "source": [
    "The p-value is less than alpha and we reject the null hypothesis. There is evidence to support that patients who have been married has an association with stroke. I believe that marriage is a driver of stroke. Adding an encoded version of this feature to the model will likely increase the mode's accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925e831b",
   "metadata": {},
   "source": [
    "# Exploration Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284d287c",
   "metadata": {},
   "source": [
    "* 4.86% of our patient populatin has suffer a stroke\n",
    "* Patients with hypertension have an increase in stroke rate\n",
    "* Patients with heart disease have an increase in stroke rate\n",
    "* Patients wih heart disease have a larger increase in stroke rate than patients with hypertension\n",
    "* The male gender of our patients that have heart disease have a higher stroke rate than the female gender\n",
    "* The patients age is a driver of stroke\n",
    "* Patients who have been married have a slight increase in stroke rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13691ba",
   "metadata": {},
   "source": [
    "# Features that will be included in my model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2f55bf",
   "metadata": {},
   "source": [
    "The following features where found significant trough exploration and statistical testing and will be kept for modeling:\n",
    "* hypertension \n",
    "* heart_disease\n",
    "* gender_Male (encoded)\n",
    "* age (scaled)\n",
    "* ever_married_Yes (encoded)\n",
    "\n",
    "Target variable: Stroke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "472c63d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save features kept for modeling\n",
    "modeling_features = ['hypertension','heart_disease','gender_Male','age','ever_married_Yes', 'stroke']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1e8571",
   "metadata": {},
   "source": [
    "# Features that will be not included in my model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5515f46",
   "metadata": {},
   "source": [
    "The following features will not be used for this iteration of modeling:\n",
    "* work_type\n",
    "* residence_type\n",
    "* avg_glucose_level\n",
    "* bmi\n",
    "* smoking_status\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b8791e",
   "metadata": {},
   "source": [
    "# Modeling:\n",
    "* Accuracy is the metric use in the models.\n",
    "    * Accuracy helps gauge the percentage of correct predictions\n",
    "* Patients who have suffer a stroke makeup 4.86% of the data\n",
    "* 95.14% will be the baseline\n",
    "* I will evaluate my top model on train and validate data\n",
    "* The model that performs the best will then be evaluated on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "364ee496",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train,y_train,X_validate,y_validate, X_test, y_test= m.model_prep(train,validate,test,modeling_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa411091",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1227,), (1227, 5), (2861, 5), (2861,), (1022, 5), (1022,))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_validate.shape,X_validate.shape,X_train.shape,y_train.shape,X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ab4cb4",
   "metadata": {},
   "source": [
    "X_train['baseline_prediction'] = int(baseline)\n",
    "X_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54e1b1c",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "#Loop for models\n",
    "for i in range(1, 21):\n",
    "    # Make the model\n",
    "    tree = DecisionTreeClassifier(max_depth=i,min_samples_split = 5, random_state=123)\n",
    "\n",
    "    # Fit the model (on train and only train)\n",
    "    tree = tree.fit(X_train, y_train)\n",
    "\n",
    "    # Use the model\n",
    "    # We'll evaluate the model's performance on train, first\n",
    "    y_predictions = tree.predict(X_train)\n",
    "\n",
    "    # Produce the classification report on the actual y values and this model's predicted y values\n",
    "    report = classification_report(y_train, y_predictions, output_dict=True)\n",
    "    print(f\"Tree with max depth of {i}\")\n",
    "    print(pd.DataFrame(report))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb8c54d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2020a371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1227,), (1227, 5), (2861, 5), (2861,))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_validate.shape,X_validate.shape,X_train.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "59f65f15",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_depth</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>validate_accuracy</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>validate_recall</th>\n",
       "      <th>accuracy_difference</th>\n",
       "      <th>recall_difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.951100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.951100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.951100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.951100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.951100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.952115</td>\n",
       "      <td>0.951915</td>\n",
       "      <td>0.035971</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>0.002638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.953513</td>\n",
       "      <td>0.947840</td>\n",
       "      <td>0.071942</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.005672</td>\n",
       "      <td>0.055276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.955610</td>\n",
       "      <td>0.941320</td>\n",
       "      <td>0.129496</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.014290</td>\n",
       "      <td>0.096163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.955959</td>\n",
       "      <td>0.943765</td>\n",
       "      <td>0.136691</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.012194</td>\n",
       "      <td>0.103357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.956309</td>\n",
       "      <td>0.941320</td>\n",
       "      <td>0.129496</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.014989</td>\n",
       "      <td>0.096163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0.957358</td>\n",
       "      <td>0.938875</td>\n",
       "      <td>0.158273</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.018482</td>\n",
       "      <td>0.124940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0.957707</td>\n",
       "      <td>0.938875</td>\n",
       "      <td>0.158273</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.018832</td>\n",
       "      <td>0.124940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0.957707</td>\n",
       "      <td>0.938060</td>\n",
       "      <td>0.158273</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.019647</td>\n",
       "      <td>0.124940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0.957707</td>\n",
       "      <td>0.938060</td>\n",
       "      <td>0.158273</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.019647</td>\n",
       "      <td>0.124940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0.957707</td>\n",
       "      <td>0.938060</td>\n",
       "      <td>0.158273</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.019647</td>\n",
       "      <td>0.124940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>0.957707</td>\n",
       "      <td>0.938060</td>\n",
       "      <td>0.158273</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.019647</td>\n",
       "      <td>0.124940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>0.957707</td>\n",
       "      <td>0.938060</td>\n",
       "      <td>0.158273</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.019647</td>\n",
       "      <td>0.124940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>0.957707</td>\n",
       "      <td>0.938060</td>\n",
       "      <td>0.158273</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.019647</td>\n",
       "      <td>0.124940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>0.957707</td>\n",
       "      <td>0.938060</td>\n",
       "      <td>0.158273</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.019647</td>\n",
       "      <td>0.124940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>0.957707</td>\n",
       "      <td>0.938060</td>\n",
       "      <td>0.158273</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.019647</td>\n",
       "      <td>0.124940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>0.957707</td>\n",
       "      <td>0.938060</td>\n",
       "      <td>0.158273</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.019647</td>\n",
       "      <td>0.124940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>0.957707</td>\n",
       "      <td>0.938060</td>\n",
       "      <td>0.158273</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.019647</td>\n",
       "      <td>0.124940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>0.957707</td>\n",
       "      <td>0.938060</td>\n",
       "      <td>0.158273</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.019647</td>\n",
       "      <td>0.124940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>0.957707</td>\n",
       "      <td>0.938060</td>\n",
       "      <td>0.158273</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.019647</td>\n",
       "      <td>0.124940</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    max_depth  train_accuracy  validate_accuracy  train_recall  \\\n",
       "0           1        0.951416           0.951100      0.000000   \n",
       "1           2        0.951416           0.951100      0.000000   \n",
       "2           3        0.951416           0.951100      0.000000   \n",
       "3           4        0.951416           0.951100      0.000000   \n",
       "4           5        0.951416           0.951100      0.000000   \n",
       "5           6        0.952115           0.951915      0.035971   \n",
       "6           7        0.953513           0.947840      0.071942   \n",
       "7           8        0.955610           0.941320      0.129496   \n",
       "8           9        0.955959           0.943765      0.136691   \n",
       "9          10        0.956309           0.941320      0.129496   \n",
       "10         11        0.957358           0.938875      0.158273   \n",
       "11         12        0.957707           0.938875      0.158273   \n",
       "12         13        0.957707           0.938060      0.158273   \n",
       "13         14        0.957707           0.938060      0.158273   \n",
       "14         15        0.957707           0.938060      0.158273   \n",
       "15         16        0.957707           0.938060      0.158273   \n",
       "16         17        0.957707           0.938060      0.158273   \n",
       "17         18        0.957707           0.938060      0.158273   \n",
       "18         19        0.957707           0.938060      0.158273   \n",
       "19         20        0.957707           0.938060      0.158273   \n",
       "20         21        0.957707           0.938060      0.158273   \n",
       "21         22        0.957707           0.938060      0.158273   \n",
       "22         23        0.957707           0.938060      0.158273   \n",
       "23         24        0.957707           0.938060      0.158273   \n",
       "\n",
       "    validate_recall  accuracy_difference  recall_difference  \n",
       "0          0.000000             0.000315           0.000000  \n",
       "1          0.000000             0.000315           0.000000  \n",
       "2          0.000000             0.000315           0.000000  \n",
       "3          0.000000             0.000315           0.000000  \n",
       "4          0.000000             0.000315           0.000000  \n",
       "5          0.033333             0.000199           0.002638  \n",
       "6          0.016667             0.005672           0.055276  \n",
       "7          0.033333             0.014290           0.096163  \n",
       "8          0.033333             0.012194           0.103357  \n",
       "9          0.033333             0.014989           0.096163  \n",
       "10         0.033333             0.018482           0.124940  \n",
       "11         0.033333             0.018832           0.124940  \n",
       "12         0.033333             0.019647           0.124940  \n",
       "13         0.033333             0.019647           0.124940  \n",
       "14         0.033333             0.019647           0.124940  \n",
       "15         0.033333             0.019647           0.124940  \n",
       "16         0.033333             0.019647           0.124940  \n",
       "17         0.033333             0.019647           0.124940  \n",
       "18         0.033333             0.019647           0.124940  \n",
       "19         0.033333             0.019647           0.124940  \n",
       "20         0.033333             0.019647           0.124940  \n",
       "21         0.033333             0.019647           0.124940  \n",
       "22         0.033333             0.019647           0.124940  \n",
       "23         0.033333             0.019647           0.124940  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "metrics = []\n",
    "\n",
    "for i in range(1, 25):\n",
    "    # Make the model\n",
    "    tree = DecisionTreeClassifier(max_depth=i, random_state=123)\n",
    "\n",
    "    # Fit the model (on train and only train)\n",
    "    tree = tree.fit(X_train, y_train)\n",
    "    \n",
    "\n",
    "    \n",
    "    # Use the model\n",
    "    # We'll evaluate the model's performance on train, first\n",
    "    in_sample_accuracy = tree.score(X_train, y_train)\n",
    "    \n",
    "    \n",
    "    \n",
    "    out_of_sample_accuracy = tree.score(X_validate, y_validate)\n",
    "\n",
    "    # calculate recall train\n",
    "    y_pred = tree.predict(X_train)\n",
    "    in_sample_recall= recall_score(y_train, y_pred)  \n",
    "      \n",
    "    # calculate recall validate\n",
    "    y_pred = tree.predict(X_validate)\n",
    "    out_of_sample_recall= recall_score(y_validate, y_pred)\n",
    "    \n",
    "    output = {\n",
    "        \"max_depth\": i,\n",
    "        \"train_accuracy\": in_sample_accuracy,\n",
    "        \"validate_accuracy\": out_of_sample_accuracy,\n",
    "        'train_recall': in_sample_recall,\n",
    "        'validate_recall': out_of_sample_recall\n",
    "    }\n",
    "    \n",
    "    metrics.append(output)\n",
    "    \n",
    "df = pd.DataFrame(metrics)\n",
    "df[\"accuracy_difference\"] = df.train_accuracy - df.validate_accuracy\n",
    "df[\"recall_difference\"] = df.train_recall - df.validate_recall\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3c4a73cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_depth</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>validate_accuracy</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>validate_recall</th>\n",
       "      <th>accuracy_difference</th>\n",
       "      <th>recall_difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.951100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.951100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.951100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.951100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.951100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.952115</td>\n",
       "      <td>0.951915</td>\n",
       "      <td>0.035971</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>0.002638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.953513</td>\n",
       "      <td>0.947840</td>\n",
       "      <td>0.071942</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.005672</td>\n",
       "      <td>0.055276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.955610</td>\n",
       "      <td>0.941320</td>\n",
       "      <td>0.129496</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.014290</td>\n",
       "      <td>0.096163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.956309</td>\n",
       "      <td>0.941320</td>\n",
       "      <td>0.129496</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.014989</td>\n",
       "      <td>0.096163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.955959</td>\n",
       "      <td>0.943765</td>\n",
       "      <td>0.136691</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.012194</td>\n",
       "      <td>0.103357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0.957358</td>\n",
       "      <td>0.938875</td>\n",
       "      <td>0.158273</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.018482</td>\n",
       "      <td>0.124940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0.957707</td>\n",
       "      <td>0.938875</td>\n",
       "      <td>0.158273</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.018832</td>\n",
       "      <td>0.124940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0.957707</td>\n",
       "      <td>0.938060</td>\n",
       "      <td>0.158273</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.019647</td>\n",
       "      <td>0.124940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0.957707</td>\n",
       "      <td>0.938060</td>\n",
       "      <td>0.158273</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.019647</td>\n",
       "      <td>0.124940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0.957707</td>\n",
       "      <td>0.938060</td>\n",
       "      <td>0.158273</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.019647</td>\n",
       "      <td>0.124940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>0.957707</td>\n",
       "      <td>0.938060</td>\n",
       "      <td>0.158273</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.019647</td>\n",
       "      <td>0.124940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>0.957707</td>\n",
       "      <td>0.938060</td>\n",
       "      <td>0.158273</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.019647</td>\n",
       "      <td>0.124940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>0.957707</td>\n",
       "      <td>0.938060</td>\n",
       "      <td>0.158273</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.019647</td>\n",
       "      <td>0.124940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>0.957707</td>\n",
       "      <td>0.938060</td>\n",
       "      <td>0.158273</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.019647</td>\n",
       "      <td>0.124940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>0.957707</td>\n",
       "      <td>0.938060</td>\n",
       "      <td>0.158273</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.019647</td>\n",
       "      <td>0.124940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>0.957707</td>\n",
       "      <td>0.938060</td>\n",
       "      <td>0.158273</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.019647</td>\n",
       "      <td>0.124940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>0.957707</td>\n",
       "      <td>0.938060</td>\n",
       "      <td>0.158273</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.019647</td>\n",
       "      <td>0.124940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>0.957707</td>\n",
       "      <td>0.938060</td>\n",
       "      <td>0.158273</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.019647</td>\n",
       "      <td>0.124940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>0.957707</td>\n",
       "      <td>0.938060</td>\n",
       "      <td>0.158273</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.019647</td>\n",
       "      <td>0.124940</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    max_depth  train_accuracy  validate_accuracy  train_recall  \\\n",
       "0           1        0.951416           0.951100      0.000000   \n",
       "1           2        0.951416           0.951100      0.000000   \n",
       "2           3        0.951416           0.951100      0.000000   \n",
       "3           4        0.951416           0.951100      0.000000   \n",
       "4           5        0.951416           0.951100      0.000000   \n",
       "5           6        0.952115           0.951915      0.035971   \n",
       "6           7        0.953513           0.947840      0.071942   \n",
       "7           8        0.955610           0.941320      0.129496   \n",
       "9          10        0.956309           0.941320      0.129496   \n",
       "8           9        0.955959           0.943765      0.136691   \n",
       "10         11        0.957358           0.938875      0.158273   \n",
       "11         12        0.957707           0.938875      0.158273   \n",
       "12         13        0.957707           0.938060      0.158273   \n",
       "13         14        0.957707           0.938060      0.158273   \n",
       "14         15        0.957707           0.938060      0.158273   \n",
       "15         16        0.957707           0.938060      0.158273   \n",
       "16         17        0.957707           0.938060      0.158273   \n",
       "17         18        0.957707           0.938060      0.158273   \n",
       "18         19        0.957707           0.938060      0.158273   \n",
       "19         20        0.957707           0.938060      0.158273   \n",
       "20         21        0.957707           0.938060      0.158273   \n",
       "21         22        0.957707           0.938060      0.158273   \n",
       "22         23        0.957707           0.938060      0.158273   \n",
       "23         24        0.957707           0.938060      0.158273   \n",
       "\n",
       "    validate_recall  accuracy_difference  recall_difference  \n",
       "0          0.000000             0.000315           0.000000  \n",
       "1          0.000000             0.000315           0.000000  \n",
       "2          0.000000             0.000315           0.000000  \n",
       "3          0.000000             0.000315           0.000000  \n",
       "4          0.000000             0.000315           0.000000  \n",
       "5          0.033333             0.000199           0.002638  \n",
       "6          0.016667             0.005672           0.055276  \n",
       "7          0.033333             0.014290           0.096163  \n",
       "9          0.033333             0.014989           0.096163  \n",
       "8          0.033333             0.012194           0.103357  \n",
       "10         0.033333             0.018482           0.124940  \n",
       "11         0.033333             0.018832           0.124940  \n",
       "12         0.033333             0.019647           0.124940  \n",
       "13         0.033333             0.019647           0.124940  \n",
       "14         0.033333             0.019647           0.124940  \n",
       "15         0.033333             0.019647           0.124940  \n",
       "16         0.033333             0.019647           0.124940  \n",
       "17         0.033333             0.019647           0.124940  \n",
       "18         0.033333             0.019647           0.124940  \n",
       "19         0.033333             0.019647           0.124940  \n",
       "20         0.033333             0.019647           0.124940  \n",
       "21         0.033333             0.019647           0.124940  \n",
       "22         0.033333             0.019647           0.124940  \n",
       "23         0.033333             0.019647           0.124940  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(by=['recall_difference', 'train_recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "209392c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df.max_depth, df.train_accuracy, marker = 'o', label = 'Train')\n",
    "plt.plot(df.max_depth, df.validate_accuracy, marker = 'o', label = 'Validate')\n",
    "\n",
    "\n",
    "#plt.plot(df_2.max_depth, df_2.validate_accuracy, marker = 'x', label = 'Validate')\n",
    "\n",
    "plt.title('Overfitting Occurs at Higher Values for Max Depth')\n",
    "plt.xlabel('Max Depth')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "92cbde95",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree with min sample leaf 1\n",
      "                     0           1  accuracy    macro avg  weighted avg\n",
      "precision     0.957379    0.818182  0.956309     0.887781      0.950617\n",
      "recall        0.998530    0.129496  0.956309     0.564013      0.956309\n",
      "f1-score      0.977522    0.223602  0.956309     0.600562      0.940893\n",
      "support    2722.000000  139.000000  0.956309  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 2\n",
      "                     0           1  accuracy    macro avg  weighted avg\n",
      "precision     0.954386    0.818182  0.953862     0.886284      0.947769\n",
      "recall        0.999265    0.064748  0.953862     0.532007      0.953862\n",
      "f1-score      0.976310    0.120000  0.953862     0.548155      0.934707\n",
      "support    2722.000000  139.000000  0.953862  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 3\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 4\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 5\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 6\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 7\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 8\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 9\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 10\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 11\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 12\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 13\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 14\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 15\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 16\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 17\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 18\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 19\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 20\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_depth</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>validate_accuracy</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>validate_recall</th>\n",
       "      <th>accuracy_difference</th>\n",
       "      <th>recall_difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.956309</td>\n",
       "      <td>0.945395</td>\n",
       "      <td>0.129496</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.010914</td>\n",
       "      <td>0.112830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.953862</td>\n",
       "      <td>0.947840</td>\n",
       "      <td>0.064748</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006022</td>\n",
       "      <td>0.064748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.951100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.951100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.951100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.951100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.951100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.951100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.951100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.951100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.951100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.951100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.951100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.951100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.951100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.951100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.951100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.951100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.951100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.951100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    max_depth  train_accuracy  validate_accuracy  train_recall  \\\n",
       "0           1        0.956309           0.945395      0.129496   \n",
       "1           2        0.953862           0.947840      0.064748   \n",
       "2           3        0.951416           0.951100      0.000000   \n",
       "3           4        0.951416           0.951100      0.000000   \n",
       "4           5        0.951416           0.951100      0.000000   \n",
       "5           6        0.951416           0.951100      0.000000   \n",
       "6           7        0.951416           0.951100      0.000000   \n",
       "7           8        0.951416           0.951100      0.000000   \n",
       "8           9        0.951416           0.951100      0.000000   \n",
       "9          10        0.951416           0.951100      0.000000   \n",
       "10         11        0.951416           0.951100      0.000000   \n",
       "11         12        0.951416           0.951100      0.000000   \n",
       "12         13        0.951416           0.951100      0.000000   \n",
       "13         14        0.951416           0.951100      0.000000   \n",
       "14         15        0.951416           0.951100      0.000000   \n",
       "15         16        0.951416           0.951100      0.000000   \n",
       "16         17        0.951416           0.951100      0.000000   \n",
       "17         18        0.951416           0.951100      0.000000   \n",
       "18         19        0.951416           0.951100      0.000000   \n",
       "19         20        0.951416           0.951100      0.000000   \n",
       "\n",
       "    validate_recall  accuracy_difference  recall_difference  \n",
       "0          0.016667             0.010914           0.112830  \n",
       "1          0.000000             0.006022           0.064748  \n",
       "2          0.000000             0.000315           0.000000  \n",
       "3          0.000000             0.000315           0.000000  \n",
       "4          0.000000             0.000315           0.000000  \n",
       "5          0.000000             0.000315           0.000000  \n",
       "6          0.000000             0.000315           0.000000  \n",
       "7          0.000000             0.000315           0.000000  \n",
       "8          0.000000             0.000315           0.000000  \n",
       "9          0.000000             0.000315           0.000000  \n",
       "10         0.000000             0.000315           0.000000  \n",
       "11         0.000000             0.000315           0.000000  \n",
       "12         0.000000             0.000315           0.000000  \n",
       "13         0.000000             0.000315           0.000000  \n",
       "14         0.000000             0.000315           0.000000  \n",
       "15         0.000000             0.000315           0.000000  \n",
       "16         0.000000             0.000315           0.000000  \n",
       "17         0.000000             0.000315           0.000000  \n",
       "18         0.000000             0.000315           0.000000  \n",
       "19         0.000000             0.000315           0.000000  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Random forest changing leaf\n",
    "\n",
    "metrics=[]\n",
    "for i in range(1, 21):\n",
    "  \n",
    "    # Make the model\n",
    "    random_forest = RandomForestClassifier(max_depth=8, min_samples_leaf = i , random_state=123)\n",
    "\n",
    "    # Fit the model (on train and only train)\n",
    "    random_forest = random_forest.fit(X_train, y_train)\n",
    "\n",
    "    # Use the model\n",
    "    # We'll evaluate the model's performance on train, first\n",
    "    y_pred = random_forest.predict(X_train)\n",
    "\n",
    "    # Produce the classification report on the actual y values and this model's predicted y values\n",
    "    report = classification_report(y_train, y_pred, output_dict=True)\n",
    "    print(f\"Tree with min sample leaf {i}\")\n",
    "    print(pd.DataFrame(report))\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    # create data frame\n",
    "        # calculate recall train\n",
    "    y_pred = random_forest.predict(X_train)\n",
    "    in_sample_recall= recall_score(y_train, y_pred)\n",
    "    \n",
    "    # Use the model\n",
    "    # We'll evaluate the model's performance on train, first\n",
    "    in_sample_accuracy = random_forest.score(X_train, y_train)\n",
    "    \n",
    "    \n",
    "    \n",
    "    out_of_sample_accuracy = random_forest.score(X_validate, y_validate)\n",
    "\n",
    "    \n",
    "      \n",
    "    # calculate recall validate\n",
    "    y_pred = random_forest.predict(X_validate)\n",
    "    out_of_sample_recall= recall_score(y_validate, y_pred)\n",
    "    \n",
    "    output = {\n",
    "        \"max_depth\": i,\n",
    "        \"train_accuracy\": in_sample_accuracy,\n",
    "        \"validate_accuracy\": out_of_sample_accuracy,\n",
    "        'train_recall': in_sample_recall,\n",
    "        'validate_recall': out_of_sample_recall\n",
    "    }\n",
    "    \n",
    "    metrics.append(output)\n",
    "    \n",
    "df6 = pd.DataFrame(metrics)\n",
    "df6[\"accuracy_difference\"] = df6.train_accuracy - df6.validate_accuracy\n",
    "df6[\"recall_difference\"] = df6.train_recall - df6.validate_recall\n",
    "df6\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d6b1e0c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_depth</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>validate_accuracy</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>validate_recall</th>\n",
       "      <th>accuracy_difference</th>\n",
       "      <th>recall_difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.951100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.951100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.951100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.951100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.951100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.951100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.951100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.951100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.951100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.951100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.951100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.951100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.951100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.951100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.951100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.951100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.951100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.951100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.954561</td>\n",
       "      <td>0.947840</td>\n",
       "      <td>0.086331</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006721</td>\n",
       "      <td>0.086331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.957707</td>\n",
       "      <td>0.940505</td>\n",
       "      <td>0.172662</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.017202</td>\n",
       "      <td>0.155995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    max_depth  train_accuracy  validate_accuracy  train_recall  \\\n",
       "2           3        0.951416           0.951100      0.000000   \n",
       "3           4        0.951416           0.951100      0.000000   \n",
       "4           5        0.951416           0.951100      0.000000   \n",
       "5           6        0.951416           0.951100      0.000000   \n",
       "6           7        0.951416           0.951100      0.000000   \n",
       "7           8        0.951416           0.951100      0.000000   \n",
       "8           9        0.951416           0.951100      0.000000   \n",
       "9          10        0.951416           0.951100      0.000000   \n",
       "10         11        0.951416           0.951100      0.000000   \n",
       "11         12        0.951416           0.951100      0.000000   \n",
       "12         13        0.951416           0.951100      0.000000   \n",
       "13         14        0.951416           0.951100      0.000000   \n",
       "14         15        0.951416           0.951100      0.000000   \n",
       "15         16        0.951416           0.951100      0.000000   \n",
       "16         17        0.951416           0.951100      0.000000   \n",
       "17         18        0.951416           0.951100      0.000000   \n",
       "18         19        0.951416           0.951100      0.000000   \n",
       "19         20        0.951416           0.951100      0.000000   \n",
       "1           2        0.954561           0.947840      0.086331   \n",
       "0           1        0.957707           0.940505      0.172662   \n",
       "\n",
       "    validate_recall  accuracy_difference  recall_difference  \n",
       "2          0.000000             0.000315           0.000000  \n",
       "3          0.000000             0.000315           0.000000  \n",
       "4          0.000000             0.000315           0.000000  \n",
       "5          0.000000             0.000315           0.000000  \n",
       "6          0.000000             0.000315           0.000000  \n",
       "7          0.000000             0.000315           0.000000  \n",
       "8          0.000000             0.000315           0.000000  \n",
       "9          0.000000             0.000315           0.000000  \n",
       "10         0.000000             0.000315           0.000000  \n",
       "11         0.000000             0.000315           0.000000  \n",
       "12         0.000000             0.000315           0.000000  \n",
       "13         0.000000             0.000315           0.000000  \n",
       "14         0.000000             0.000315           0.000000  \n",
       "15         0.000000             0.000315           0.000000  \n",
       "16         0.000000             0.000315           0.000000  \n",
       "17         0.000000             0.000315           0.000000  \n",
       "18         0.000000             0.000315           0.000000  \n",
       "19         0.000000             0.000315           0.000000  \n",
       "1          0.000000             0.006721           0.086331  \n",
       "0          0.016667             0.017202           0.155995  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df6.sort_values(by=['validate_accuracy','validate_recall'],ascending= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "78eda56c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max depth 10\n",
      "Tree with min sample leaf 1\n",
      "max depth 10\n",
      "                     0           1  accuracy    macro avg  weighted avg\n",
      "precision     0.959378    0.800000  0.957707     0.879689      0.951635\n",
      "recall        0.997796    0.172662  0.957707     0.585229      0.957707\n",
      "f1-score      0.978210    0.284024  0.957707     0.631117      0.944483\n",
      "support    2722.000000  139.000000  0.957707  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 1\n",
      "max depth 10\n",
      "                     0          1  accuracy    macro avg  weighted avg\n",
      "precision     0.951320   0.066667  0.940505     0.508993      0.908061\n",
      "recall        0.988003   0.016667  0.940505     0.502335      0.940505\n",
      "f1-score      0.969315   0.026667  0.940505     0.497991      0.923220\n",
      "support    1167.000000  60.000000  0.940505  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 2\n",
      "max depth 10\n",
      "                     0           1  accuracy    macro avg  weighted avg\n",
      "precision     0.955376    0.800000  0.954561     0.877688      0.947827\n",
      "recall        0.998898    0.086331  0.954561     0.542614      0.954561\n",
      "f1-score      0.976652    0.155844  0.954561     0.566248      0.936774\n",
      "support    2722.000000  139.000000  0.954561  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 2\n",
      "max depth 10\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.950940   0.0   0.94784     0.475470      0.904440\n",
      "recall        0.996572   0.0   0.94784     0.498286      0.947840\n",
      "f1-score      0.973222   0.0   0.94784     0.486611      0.925631\n",
      "support    1167.000000  60.0   0.94784  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 3\n",
      "max depth 10\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 3\n",
      "max depth 10\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 4\n",
      "max depth 10\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 4\n",
      "max depth 10\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 5\n",
      "max depth 10\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 5\n",
      "max depth 10\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 6\n",
      "max depth 10\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 6\n",
      "max depth 10\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 7\n",
      "max depth 10\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 7\n",
      "max depth 10\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 8\n",
      "max depth 10\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 8\n",
      "max depth 10\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 9\n",
      "max depth 10\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 9\n",
      "max depth 10\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 10\n",
      "max depth 10\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 10\n",
      "max depth 10\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 11\n",
      "max depth 10\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 11\n",
      "max depth 10\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree with min sample leaf 12\n",
      "max depth 10\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 12\n",
      "max depth 10\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 13\n",
      "max depth 10\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 13\n",
      "max depth 10\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 14\n",
      "max depth 10\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 14\n",
      "max depth 10\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 15\n",
      "max depth 10\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 15\n",
      "max depth 10\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 16\n",
      "max depth 10\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 16\n",
      "max depth 10\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 17\n",
      "max depth 10\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 17\n",
      "max depth 10\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 18\n",
      "max depth 10\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 18\n",
      "max depth 10\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 19\n",
      "max depth 10\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 19\n",
      "max depth 10\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 20\n",
      "max depth 10\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 20\n",
      "max depth 10\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "max depth 9\n",
      "Tree with min sample leaf 1\n",
      "max depth 9\n",
      "                     0           1  accuracy    macro avg  weighted avg\n",
      "precision     0.958054    0.833333  0.957008     0.895694      0.951995\n",
      "recall        0.998530    0.143885  0.957008     0.571208      0.957008\n",
      "f1-score      0.977874    0.245399  0.957008     0.611636      0.942287\n",
      "support    2722.000000  139.000000  0.957008  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 1\n",
      "max depth 9\n",
      "                     0          1  accuracy    macro avg  weighted avg\n",
      "precision     0.951520   0.100000   0.94458     0.525760      0.909881\n",
      "recall        0.992288   0.016667   0.94458     0.504477      0.944580\n",
      "f1-score      0.971477   0.028571   0.94458     0.500024      0.925369\n",
      "support    1167.000000  60.000000   0.94458  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 2\n",
      "max depth 9\n",
      "                     0           1  accuracy    macro avg  weighted avg\n",
      "precision     0.955040    0.785714  0.954212     0.870377      0.946814\n",
      "recall        0.998898    0.079137  0.954212     0.539017      0.954212\n",
      "f1-score      0.976477    0.143791  0.954212     0.560134      0.936021\n",
      "support    2722.000000  139.000000  0.954212  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 2\n",
      "max depth 9\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.950940   0.0   0.94784     0.475470      0.904440\n",
      "recall        0.996572   0.0   0.94784     0.498286      0.947840\n",
      "f1-score      0.973222   0.0   0.94784     0.486611      0.925631\n",
      "support    1167.000000  60.0   0.94784  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 3\n",
      "max depth 9\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 3\n",
      "max depth 9\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree with min sample leaf 4\n",
      "max depth 9\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 4\n",
      "max depth 9\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 5\n",
      "max depth 9\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 5\n",
      "max depth 9\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 6\n",
      "max depth 9\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 6\n",
      "max depth 9\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 7\n",
      "max depth 9\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 7\n",
      "max depth 9\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 8\n",
      "max depth 9\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 8\n",
      "max depth 9\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 9\n",
      "max depth 9\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 9\n",
      "max depth 9\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 10\n",
      "max depth 9\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 10\n",
      "max depth 9\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 11\n",
      "max depth 9\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 11\n",
      "max depth 9\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 12\n",
      "max depth 9\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 12\n",
      "max depth 9\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 13\n",
      "max depth 9\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 13\n",
      "max depth 9\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 14\n",
      "max depth 9\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 14\n",
      "max depth 9\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 15\n",
      "max depth 9\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 15\n",
      "max depth 9\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree with min sample leaf 16\n",
      "max depth 9\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 16\n",
      "max depth 9\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 17\n",
      "max depth 9\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 17\n",
      "max depth 9\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 18\n",
      "max depth 9\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 18\n",
      "max depth 9\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 19\n",
      "max depth 9\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 19\n",
      "max depth 9\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 20\n",
      "max depth 9\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 20\n",
      "max depth 9\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "max depth 8\n",
      "Tree with min sample leaf 1\n",
      "max depth 8\n",
      "                     0           1  accuracy    macro avg  weighted avg\n",
      "precision     0.957379    0.818182  0.956309     0.887781      0.950617\n",
      "recall        0.998530    0.129496  0.956309     0.564013      0.956309\n",
      "f1-score      0.977522    0.223602  0.956309     0.600562      0.940893\n",
      "support    2722.000000  139.000000  0.956309  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 1\n",
      "max depth 8\n",
      "                     0          1  accuracy    macro avg  weighted avg\n",
      "precision     0.951560   0.111111  0.945395     0.531336      0.910462\n",
      "recall        0.993145   0.016667  0.945395     0.504906      0.945395\n",
      "f1-score      0.971908   0.028986  0.945395     0.500447      0.925799\n",
      "support    1167.000000  60.000000  0.945395  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 2\n",
      "max depth 8\n",
      "                     0           1  accuracy    macro avg  weighted avg\n",
      "precision     0.954386    0.818182  0.953862     0.886284      0.947769\n",
      "recall        0.999265    0.064748  0.953862     0.532007      0.953862\n",
      "f1-score      0.976310    0.120000  0.953862     0.548155      0.934707\n",
      "support    2722.000000  139.000000  0.953862  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 2\n",
      "max depth 8\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.950940   0.0   0.94784     0.475470      0.904440\n",
      "recall        0.996572   0.0   0.94784     0.498286      0.947840\n",
      "f1-score      0.973222   0.0   0.94784     0.486611      0.925631\n",
      "support    1167.000000  60.0   0.94784  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 3\n",
      "max depth 8\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 3\n",
      "max depth 8\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 4\n",
      "max depth 8\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 4\n",
      "max depth 8\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 5\n",
      "max depth 8\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 5\n",
      "max depth 8\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 6\n",
      "max depth 8\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 6\n",
      "max depth 8\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 7\n",
      "max depth 8\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 7\n",
      "max depth 8\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree with min sample leaf 8\n",
      "max depth 8\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 8\n",
      "max depth 8\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 9\n",
      "max depth 8\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 9\n",
      "max depth 8\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 10\n",
      "max depth 8\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 10\n",
      "max depth 8\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 11\n",
      "max depth 8\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 11\n",
      "max depth 8\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 12\n",
      "max depth 8\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 12\n",
      "max depth 8\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 13\n",
      "max depth 8\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 13\n",
      "max depth 8\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 14\n",
      "max depth 8\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 14\n",
      "max depth 8\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 15\n",
      "max depth 8\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 15\n",
      "max depth 8\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 16\n",
      "max depth 8\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 16\n",
      "max depth 8\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 17\n",
      "max depth 8\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 17\n",
      "max depth 8\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 18\n",
      "max depth 8\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 18\n",
      "max depth 8\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 19\n",
      "max depth 8\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 19\n",
      "max depth 8\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree with min sample leaf 20\n",
      "max depth 8\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 20\n",
      "max depth 8\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "max depth 7\n",
      "Tree with min sample leaf 1\n",
      "max depth 7\n",
      "                     0           1  accuracy    macro avg  weighted avg\n",
      "precision     0.955040    0.785714  0.954212     0.870377      0.946814\n",
      "recall        0.998898    0.079137  0.954212     0.539017      0.954212\n",
      "f1-score      0.976477    0.143791  0.954212     0.560134      0.936021\n",
      "support    2722.000000  139.000000  0.954212  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 1\n",
      "max depth 7\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.950940   0.0   0.94784     0.475470      0.904440\n",
      "recall        0.996572   0.0   0.94784     0.498286      0.947840\n",
      "f1-score      0.973222   0.0   0.94784     0.486611      0.925631\n",
      "support    1167.000000  60.0   0.94784  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 2\n",
      "max depth 7\n",
      "                     0           1  accuracy    macro avg  weighted avg\n",
      "precision     0.952064    0.666667  0.951765     0.809366      0.938199\n",
      "recall        0.999633    0.014388  0.951765     0.507011      0.951765\n",
      "f1-score      0.975269    0.028169  0.951765     0.501719      0.929255\n",
      "support    2722.000000  139.000000  0.951765  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 2\n",
      "max depth 7\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 3\n",
      "max depth 7\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 3\n",
      "max depth 7\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 4\n",
      "max depth 7\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 4\n",
      "max depth 7\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 5\n",
      "max depth 7\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 5\n",
      "max depth 7\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 6\n",
      "max depth 7\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 6\n",
      "max depth 7\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 7\n",
      "max depth 7\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 7\n",
      "max depth 7\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 8\n",
      "max depth 7\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 8\n",
      "max depth 7\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 9\n",
      "max depth 7\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 9\n",
      "max depth 7\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 10\n",
      "max depth 7\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 10\n",
      "max depth 7\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 11\n",
      "max depth 7\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 11\n",
      "max depth 7\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree with min sample leaf 12\n",
      "max depth 7\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 12\n",
      "max depth 7\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 13\n",
      "max depth 7\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 13\n",
      "max depth 7\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 14\n",
      "max depth 7\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 14\n",
      "max depth 7\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 15\n",
      "max depth 7\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 15\n",
      "max depth 7\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 16\n",
      "max depth 7\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 16\n",
      "max depth 7\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 17\n",
      "max depth 7\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 17\n",
      "max depth 7\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 18\n",
      "max depth 7\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 18\n",
      "max depth 7\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 19\n",
      "max depth 7\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 19\n",
      "max depth 7\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 20\n",
      "max depth 7\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 20\n",
      "max depth 7\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "max depth 6\n",
      "Tree with min sample leaf 1\n",
      "max depth 6\n",
      "                     0           1  accuracy    macro avg  weighted avg\n",
      "precision     0.952731    0.800000  0.952464     0.876366      0.945311\n",
      "recall        0.999633    0.028777  0.952464     0.514205      0.952464\n",
      "f1-score      0.975619    0.055556  0.952464     0.515587      0.930918\n",
      "support    2722.000000  139.000000  0.952464  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 1\n",
      "max depth 6\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 2\n",
      "max depth 6\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 2\n",
      "max depth 6\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree with min sample leaf 3\n",
      "max depth 6\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 3\n",
      "max depth 6\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 4\n",
      "max depth 6\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 4\n",
      "max depth 6\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 5\n",
      "max depth 6\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 5\n",
      "max depth 6\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 6\n",
      "max depth 6\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 6\n",
      "max depth 6\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 7\n",
      "max depth 6\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 7\n",
      "max depth 6\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 8\n",
      "max depth 6\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 8\n",
      "max depth 6\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 9\n",
      "max depth 6\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 9\n",
      "max depth 6\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 10\n",
      "max depth 6\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 10\n",
      "max depth 6\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 11\n",
      "max depth 6\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 11\n",
      "max depth 6\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 12\n",
      "max depth 6\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 12\n",
      "max depth 6\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 13\n",
      "max depth 6\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 13\n",
      "max depth 6\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 14\n",
      "max depth 6\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 14\n",
      "max depth 6\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree with min sample leaf 15\n",
      "max depth 6\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 15\n",
      "max depth 6\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 16\n",
      "max depth 6\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 16\n",
      "max depth 6\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 17\n",
      "max depth 6\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 17\n",
      "max depth 6\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 18\n",
      "max depth 6\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 18\n",
      "max depth 6\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 19\n",
      "max depth 6\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 19\n",
      "max depth 6\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 20\n",
      "max depth 6\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 20\n",
      "max depth 6\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "max depth 5\n",
      "Tree with min sample leaf 1\n",
      "max depth 5\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 1\n",
      "max depth 5\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 2\n",
      "max depth 5\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 2\n",
      "max depth 5\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 3\n",
      "max depth 5\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 3\n",
      "max depth 5\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 4\n",
      "max depth 5\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 4\n",
      "max depth 5\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 5\n",
      "max depth 5\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 5\n",
      "max depth 5\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree with min sample leaf 6\n",
      "max depth 5\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 6\n",
      "max depth 5\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 7\n",
      "max depth 5\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 7\n",
      "max depth 5\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 8\n",
      "max depth 5\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 8\n",
      "max depth 5\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 9\n",
      "max depth 5\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 9\n",
      "max depth 5\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 10\n",
      "max depth 5\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 10\n",
      "max depth 5\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 11\n",
      "max depth 5\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 11\n",
      "max depth 5\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 12\n",
      "max depth 5\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 12\n",
      "max depth 5\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 13\n",
      "max depth 5\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 13\n",
      "max depth 5\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 14\n",
      "max depth 5\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 14\n",
      "max depth 5\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 15\n",
      "max depth 5\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 15\n",
      "max depth 5\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 16\n",
      "max depth 5\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 16\n",
      "max depth 5\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 17\n",
      "max depth 5\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 17\n",
      "max depth 5\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 18\n",
      "max depth 5\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 18\n",
      "max depth 5\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree with min sample leaf 19\n",
      "max depth 5\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 19\n",
      "max depth 5\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 20\n",
      "max depth 5\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 20\n",
      "max depth 5\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "max depth 4\n",
      "Tree with min sample leaf 1\n",
      "max depth 4\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 1\n",
      "max depth 4\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 2\n",
      "max depth 4\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 2\n",
      "max depth 4\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 3\n",
      "max depth 4\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 3\n",
      "max depth 4\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 4\n",
      "max depth 4\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 4\n",
      "max depth 4\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 5\n",
      "max depth 4\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 5\n",
      "max depth 4\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 6\n",
      "max depth 4\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 6\n",
      "max depth 4\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 7\n",
      "max depth 4\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 7\n",
      "max depth 4\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 8\n",
      "max depth 4\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 8\n",
      "max depth 4\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 9\n",
      "max depth 4\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 9\n",
      "max depth 4\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree with min sample leaf 10\n",
      "max depth 4\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 10\n",
      "max depth 4\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 11\n",
      "max depth 4\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 11\n",
      "max depth 4\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 12\n",
      "max depth 4\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 12\n",
      "max depth 4\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 13\n",
      "max depth 4\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 13\n",
      "max depth 4\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 14\n",
      "max depth 4\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 14\n",
      "max depth 4\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 15\n",
      "max depth 4\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 15\n",
      "max depth 4\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 16\n",
      "max depth 4\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 16\n",
      "max depth 4\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 17\n",
      "max depth 4\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 17\n",
      "max depth 4\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 18\n",
      "max depth 4\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 18\n",
      "max depth 4\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 19\n",
      "max depth 4\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 19\n",
      "max depth 4\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 20\n",
      "max depth 4\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 20\n",
      "max depth 4\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "max depth 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree with min sample leaf 1\n",
      "max depth 3\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 1\n",
      "max depth 3\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 2\n",
      "max depth 3\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 2\n",
      "max depth 3\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 3\n",
      "max depth 3\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 3\n",
      "max depth 3\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 4\n",
      "max depth 3\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 4\n",
      "max depth 3\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 5\n",
      "max depth 3\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 5\n",
      "max depth 3\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 6\n",
      "max depth 3\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 6\n",
      "max depth 3\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 7\n",
      "max depth 3\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 7\n",
      "max depth 3\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 8\n",
      "max depth 3\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 8\n",
      "max depth 3\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 9\n",
      "max depth 3\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 9\n",
      "max depth 3\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 10\n",
      "max depth 3\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 10\n",
      "max depth 3\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 11\n",
      "max depth 3\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 11\n",
      "max depth 3\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 12\n",
      "max depth 3\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 12\n",
      "max depth 3\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree with min sample leaf 13\n",
      "max depth 3\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 13\n",
      "max depth 3\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 14\n",
      "max depth 3\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 14\n",
      "max depth 3\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 15\n",
      "max depth 3\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 15\n",
      "max depth 3\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 16\n",
      "max depth 3\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 16\n",
      "max depth 3\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 17\n",
      "max depth 3\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 17\n",
      "max depth 3\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 18\n",
      "max depth 3\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 18\n",
      "max depth 3\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 19\n",
      "max depth 3\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 19\n",
      "max depth 3\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 20\n",
      "max depth 3\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 20\n",
      "max depth 3\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "max depth 2\n",
      "Tree with min sample leaf 1\n",
      "max depth 2\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 1\n",
      "max depth 2\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 2\n",
      "max depth 2\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 2\n",
      "max depth 2\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 3\n",
      "max depth 2\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 3\n",
      "max depth 2\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 4\n",
      "max depth 2\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 4\n",
      "max depth 2\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree with min sample leaf 5\n",
      "max depth 2\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 5\n",
      "max depth 2\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 6\n",
      "max depth 2\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 6\n",
      "max depth 2\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 7\n",
      "max depth 2\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 7\n",
      "max depth 2\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 8\n",
      "max depth 2\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 8\n",
      "max depth 2\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 9\n",
      "max depth 2\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 9\n",
      "max depth 2\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 10\n",
      "max depth 2\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 10\n",
      "max depth 2\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 11\n",
      "max depth 2\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 11\n",
      "max depth 2\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 12\n",
      "max depth 2\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 12\n",
      "max depth 2\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 13\n",
      "max depth 2\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 13\n",
      "max depth 2\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 14\n",
      "max depth 2\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 14\n",
      "max depth 2\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 15\n",
      "max depth 2\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 15\n",
      "max depth 2\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 16\n",
      "max depth 2\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 16\n",
      "max depth 2\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree with min sample leaf 17\n",
      "max depth 2\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 17\n",
      "max depth 2\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 18\n",
      "max depth 2\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 18\n",
      "max depth 2\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 19\n",
      "max depth 2\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 19\n",
      "max depth 2\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 20\n",
      "max depth 2\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 20\n",
      "max depth 2\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "max depth 1\n",
      "Tree with min sample leaf 1\n",
      "max depth 1\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 1\n",
      "max depth 1\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 2\n",
      "max depth 1\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 2\n",
      "max depth 1\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 3\n",
      "max depth 1\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 3\n",
      "max depth 1\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 4\n",
      "max depth 1\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 4\n",
      "max depth 1\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 5\n",
      "max depth 1\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 5\n",
      "max depth 1\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 6\n",
      "max depth 1\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 6\n",
      "max depth 1\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 7\n",
      "max depth 1\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 7\n",
      "max depth 1\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 8\n",
      "max depth 1\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 8\n",
      "max depth 1\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree with min sample leaf 9\n",
      "max depth 1\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 9\n",
      "max depth 1\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 10\n",
      "max depth 1\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 10\n",
      "max depth 1\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 11\n",
      "max depth 1\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 11\n",
      "max depth 1\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 12\n",
      "max depth 1\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 12\n",
      "max depth 1\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 13\n",
      "max depth 1\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 13\n",
      "max depth 1\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 14\n",
      "max depth 1\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 14\n",
      "max depth 1\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 15\n",
      "max depth 1\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 15\n",
      "max depth 1\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 16\n",
      "max depth 1\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 16\n",
      "max depth 1\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 17\n",
      "max depth 1\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 17\n",
      "max depth 1\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 18\n",
      "max depth 1\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 18\n",
      "max depth 1\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 19\n",
      "max depth 1\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 19\n",
      "max depth 1\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n",
      "Tree with min sample leaf 20\n",
      "max depth 1\n",
      "                     0      1  accuracy    macro avg  weighted avg\n",
      "precision     0.951416    0.0  0.951416     0.475708      0.905192\n",
      "recall        1.000000    0.0  0.951416     0.500000      0.951416\n",
      "f1-score      0.975103    0.0  0.951416     0.487551      0.927728\n",
      "support    2722.000000  139.0  0.951416  2861.000000   2861.000000\n",
      "\n",
      "Tree with min sample leaf 20\n",
      "max depth 1\n",
      "                     0     1  accuracy    macro avg  weighted avg\n",
      "precision     0.951100   0.0    0.9511     0.475550      0.904592\n",
      "recall        1.000000   0.0    0.9511     0.500000      0.951100\n",
      "f1-score      0.974937   0.0    0.9511     0.487469      0.927263\n",
      "support    1167.000000  60.0    0.9511  1227.000000   1227.000000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random forest changing max-depth and leaf\n",
    "metrics =[]\n",
    "for h in range(10,0,-1):\n",
    "    print(f'max depth {h}')\n",
    "    for i in range(1, 21):\n",
    "  \n",
    "        # Make the model\n",
    "        random_forest = RandomForestClassifier(max_depth=h, min_samples_leaf = i , random_state=123)\n",
    "\n",
    "        # Fit the model (on train and only train)\n",
    "        random_forest = random_forest.fit(X_train, y_train)\n",
    "\n",
    "        # Use the model\n",
    "        # We'll evaluate the model's performance on train, first\n",
    "        y_pred = random_forest.predict(X_train)\n",
    "        # calculate recall validate\n",
    "\n",
    "        in_sample_recall= recall_score(y_train, y_pred)     \n",
    "\n",
    "        # Produce the classification report on the actual y values and this model's predicted y values\n",
    "        report = classification_report(y_train, y_pred, output_dict=True)\n",
    "        print(f\"Tree with min sample leaf {i}\")\n",
    "        print(f'max depth {h}')\n",
    "        print(pd.DataFrame(report))\n",
    "        print()\n",
    "        \n",
    "        \n",
    "        # calculate recall validate\n",
    "        y_pred = random_forest.predict(X_validate)\n",
    "        out_of_sample_recall= recall_score(y_validate, y_pred)\n",
    "        \n",
    "        # Produce the classification report on the actual y values and this model's predicted y values\n",
    "        report = classification_report(y_validate, y_pred, output_dict=True)\n",
    "        print(f\"Tree with min sample leaf {i}\")\n",
    "        print(f'max depth {h}')\n",
    "        print(pd.DataFrame(report))\n",
    "        print()\n",
    "        \n",
    "        output = {\n",
    "            \"max_depth\": h,\n",
    "            'leaf_min':i,\n",
    "            \"train_accuracy\": in_sample_accuracy,\n",
    "            \"validate_accuracy\": out_of_sample_accuracy,\n",
    "            'train_recall': in_sample_recall,\n",
    "            'validate_recall': out_of_sample_recall\n",
    "        }\n",
    "\n",
    "        metrics.append(output)\n",
    "\n",
    "df2 = pd.DataFrame(metrics)\n",
    "df2[\"accuracy_difference\"] = df2.train_accuracy - df2.validate_accuracy\n",
    "df2[\"recall_difference\"] = df2.train_recall - df2.validate_recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2cc5df77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_depth</th>\n",
       "      <th>leaf_min</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>validate_accuracy</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>validate_recall</th>\n",
       "      <th>accuracy_difference</th>\n",
       "      <th>recall_difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.172662</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.155995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.143885</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.127218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.129496</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.112830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.086331</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.086331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10</td>\n",
       "      <td>19</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.079137</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.079137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>9</td>\n",
       "      <td>18</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.064748</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.064748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>8</td>\n",
       "      <td>19</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.079137</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.079137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.014388</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.014388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.028777</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.028777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>6</td>\n",
       "      <td>19</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     max_depth  leaf_min  train_accuracy  validate_accuracy  train_recall  \\\n",
       "0           10         1        0.951416             0.9511      0.172662   \n",
       "20           9         1        0.951416             0.9511      0.143885   \n",
       "40           8         1        0.951416             0.9511      0.129496   \n",
       "1           10         2        0.951416             0.9511      0.086331   \n",
       "2           10         3        0.951416             0.9511      0.000000   \n",
       "3           10         4        0.951416             0.9511      0.000000   \n",
       "4           10         5        0.951416             0.9511      0.000000   \n",
       "5           10         6        0.951416             0.9511      0.000000   \n",
       "6           10         7        0.951416             0.9511      0.000000   \n",
       "7           10         8        0.951416             0.9511      0.000000   \n",
       "8           10         9        0.951416             0.9511      0.000000   \n",
       "9           10        10        0.951416             0.9511      0.000000   \n",
       "10          10        11        0.951416             0.9511      0.000000   \n",
       "11          10        12        0.951416             0.9511      0.000000   \n",
       "12          10        13        0.951416             0.9511      0.000000   \n",
       "13          10        14        0.951416             0.9511      0.000000   \n",
       "14          10        15        0.951416             0.9511      0.000000   \n",
       "15          10        16        0.951416             0.9511      0.000000   \n",
       "16          10        17        0.951416             0.9511      0.000000   \n",
       "17          10        18        0.951416             0.9511      0.000000   \n",
       "18          10        19        0.951416             0.9511      0.000000   \n",
       "19          10        20        0.951416             0.9511      0.000000   \n",
       "21           9         2        0.951416             0.9511      0.079137   \n",
       "22           9         3        0.951416             0.9511      0.000000   \n",
       "23           9         4        0.951416             0.9511      0.000000   \n",
       "24           9         5        0.951416             0.9511      0.000000   \n",
       "25           9         6        0.951416             0.9511      0.000000   \n",
       "26           9         7        0.951416             0.9511      0.000000   \n",
       "27           9         8        0.951416             0.9511      0.000000   \n",
       "28           9         9        0.951416             0.9511      0.000000   \n",
       "29           9        10        0.951416             0.9511      0.000000   \n",
       "30           9        11        0.951416             0.9511      0.000000   \n",
       "31           9        12        0.951416             0.9511      0.000000   \n",
       "32           9        13        0.951416             0.9511      0.000000   \n",
       "33           9        14        0.951416             0.9511      0.000000   \n",
       "34           9        15        0.951416             0.9511      0.000000   \n",
       "35           9        16        0.951416             0.9511      0.000000   \n",
       "36           9        17        0.951416             0.9511      0.000000   \n",
       "37           9        18        0.951416             0.9511      0.000000   \n",
       "38           9        19        0.951416             0.9511      0.000000   \n",
       "39           9        20        0.951416             0.9511      0.000000   \n",
       "41           8         2        0.951416             0.9511      0.064748   \n",
       "42           8         3        0.951416             0.9511      0.000000   \n",
       "43           8         4        0.951416             0.9511      0.000000   \n",
       "44           8         5        0.951416             0.9511      0.000000   \n",
       "45           8         6        0.951416             0.9511      0.000000   \n",
       "46           8         7        0.951416             0.9511      0.000000   \n",
       "47           8         8        0.951416             0.9511      0.000000   \n",
       "48           8         9        0.951416             0.9511      0.000000   \n",
       "49           8        10        0.951416             0.9511      0.000000   \n",
       "50           8        11        0.951416             0.9511      0.000000   \n",
       "51           8        12        0.951416             0.9511      0.000000   \n",
       "52           8        13        0.951416             0.9511      0.000000   \n",
       "53           8        14        0.951416             0.9511      0.000000   \n",
       "54           8        15        0.951416             0.9511      0.000000   \n",
       "55           8        16        0.951416             0.9511      0.000000   \n",
       "56           8        17        0.951416             0.9511      0.000000   \n",
       "57           8        18        0.951416             0.9511      0.000000   \n",
       "58           8        19        0.951416             0.9511      0.000000   \n",
       "59           8        20        0.951416             0.9511      0.000000   \n",
       "60           7         1        0.951416             0.9511      0.079137   \n",
       "61           7         2        0.951416             0.9511      0.014388   \n",
       "62           7         3        0.951416             0.9511      0.000000   \n",
       "63           7         4        0.951416             0.9511      0.000000   \n",
       "64           7         5        0.951416             0.9511      0.000000   \n",
       "65           7         6        0.951416             0.9511      0.000000   \n",
       "66           7         7        0.951416             0.9511      0.000000   \n",
       "67           7         8        0.951416             0.9511      0.000000   \n",
       "68           7         9        0.951416             0.9511      0.000000   \n",
       "69           7        10        0.951416             0.9511      0.000000   \n",
       "70           7        11        0.951416             0.9511      0.000000   \n",
       "71           7        12        0.951416             0.9511      0.000000   \n",
       "72           7        13        0.951416             0.9511      0.000000   \n",
       "73           7        14        0.951416             0.9511      0.000000   \n",
       "74           7        15        0.951416             0.9511      0.000000   \n",
       "75           7        16        0.951416             0.9511      0.000000   \n",
       "76           7        17        0.951416             0.9511      0.000000   \n",
       "77           7        18        0.951416             0.9511      0.000000   \n",
       "78           7        19        0.951416             0.9511      0.000000   \n",
       "79           7        20        0.951416             0.9511      0.000000   \n",
       "80           6         1        0.951416             0.9511      0.028777   \n",
       "81           6         2        0.951416             0.9511      0.000000   \n",
       "82           6         3        0.951416             0.9511      0.000000   \n",
       "83           6         4        0.951416             0.9511      0.000000   \n",
       "84           6         5        0.951416             0.9511      0.000000   \n",
       "85           6         6        0.951416             0.9511      0.000000   \n",
       "86           6         7        0.951416             0.9511      0.000000   \n",
       "87           6         8        0.951416             0.9511      0.000000   \n",
       "88           6         9        0.951416             0.9511      0.000000   \n",
       "89           6        10        0.951416             0.9511      0.000000   \n",
       "90           6        11        0.951416             0.9511      0.000000   \n",
       "91           6        12        0.951416             0.9511      0.000000   \n",
       "92           6        13        0.951416             0.9511      0.000000   \n",
       "93           6        14        0.951416             0.9511      0.000000   \n",
       "94           6        15        0.951416             0.9511      0.000000   \n",
       "95           6        16        0.951416             0.9511      0.000000   \n",
       "96           6        17        0.951416             0.9511      0.000000   \n",
       "97           6        18        0.951416             0.9511      0.000000   \n",
       "98           6        19        0.951416             0.9511      0.000000   \n",
       "99           6        20        0.951416             0.9511      0.000000   \n",
       "100          5         1        0.951416             0.9511      0.000000   \n",
       "101          5         2        0.951416             0.9511      0.000000   \n",
       "102          5         3        0.951416             0.9511      0.000000   \n",
       "103          5         4        0.951416             0.9511      0.000000   \n",
       "104          5         5        0.951416             0.9511      0.000000   \n",
       "105          5         6        0.951416             0.9511      0.000000   \n",
       "106          5         7        0.951416             0.9511      0.000000   \n",
       "107          5         8        0.951416             0.9511      0.000000   \n",
       "108          5         9        0.951416             0.9511      0.000000   \n",
       "109          5        10        0.951416             0.9511      0.000000   \n",
       "110          5        11        0.951416             0.9511      0.000000   \n",
       "111          5        12        0.951416             0.9511      0.000000   \n",
       "112          5        13        0.951416             0.9511      0.000000   \n",
       "113          5        14        0.951416             0.9511      0.000000   \n",
       "114          5        15        0.951416             0.9511      0.000000   \n",
       "115          5        16        0.951416             0.9511      0.000000   \n",
       "116          5        17        0.951416             0.9511      0.000000   \n",
       "117          5        18        0.951416             0.9511      0.000000   \n",
       "118          5        19        0.951416             0.9511      0.000000   \n",
       "119          5        20        0.951416             0.9511      0.000000   \n",
       "120          4         1        0.951416             0.9511      0.000000   \n",
       "121          4         2        0.951416             0.9511      0.000000   \n",
       "122          4         3        0.951416             0.9511      0.000000   \n",
       "123          4         4        0.951416             0.9511      0.000000   \n",
       "124          4         5        0.951416             0.9511      0.000000   \n",
       "125          4         6        0.951416             0.9511      0.000000   \n",
       "126          4         7        0.951416             0.9511      0.000000   \n",
       "127          4         8        0.951416             0.9511      0.000000   \n",
       "128          4         9        0.951416             0.9511      0.000000   \n",
       "129          4        10        0.951416             0.9511      0.000000   \n",
       "130          4        11        0.951416             0.9511      0.000000   \n",
       "131          4        12        0.951416             0.9511      0.000000   \n",
       "132          4        13        0.951416             0.9511      0.000000   \n",
       "133          4        14        0.951416             0.9511      0.000000   \n",
       "134          4        15        0.951416             0.9511      0.000000   \n",
       "135          4        16        0.951416             0.9511      0.000000   \n",
       "136          4        17        0.951416             0.9511      0.000000   \n",
       "137          4        18        0.951416             0.9511      0.000000   \n",
       "138          4        19        0.951416             0.9511      0.000000   \n",
       "139          4        20        0.951416             0.9511      0.000000   \n",
       "140          3         1        0.951416             0.9511      0.000000   \n",
       "141          3         2        0.951416             0.9511      0.000000   \n",
       "142          3         3        0.951416             0.9511      0.000000   \n",
       "143          3         4        0.951416             0.9511      0.000000   \n",
       "144          3         5        0.951416             0.9511      0.000000   \n",
       "145          3         6        0.951416             0.9511      0.000000   \n",
       "146          3         7        0.951416             0.9511      0.000000   \n",
       "147          3         8        0.951416             0.9511      0.000000   \n",
       "148          3         9        0.951416             0.9511      0.000000   \n",
       "149          3        10        0.951416             0.9511      0.000000   \n",
       "150          3        11        0.951416             0.9511      0.000000   \n",
       "151          3        12        0.951416             0.9511      0.000000   \n",
       "152          3        13        0.951416             0.9511      0.000000   \n",
       "153          3        14        0.951416             0.9511      0.000000   \n",
       "154          3        15        0.951416             0.9511      0.000000   \n",
       "155          3        16        0.951416             0.9511      0.000000   \n",
       "156          3        17        0.951416             0.9511      0.000000   \n",
       "157          3        18        0.951416             0.9511      0.000000   \n",
       "158          3        19        0.951416             0.9511      0.000000   \n",
       "159          3        20        0.951416             0.9511      0.000000   \n",
       "160          2         1        0.951416             0.9511      0.000000   \n",
       "161          2         2        0.951416             0.9511      0.000000   \n",
       "162          2         3        0.951416             0.9511      0.000000   \n",
       "163          2         4        0.951416             0.9511      0.000000   \n",
       "164          2         5        0.951416             0.9511      0.000000   \n",
       "165          2         6        0.951416             0.9511      0.000000   \n",
       "166          2         7        0.951416             0.9511      0.000000   \n",
       "167          2         8        0.951416             0.9511      0.000000   \n",
       "168          2         9        0.951416             0.9511      0.000000   \n",
       "169          2        10        0.951416             0.9511      0.000000   \n",
       "170          2        11        0.951416             0.9511      0.000000   \n",
       "171          2        12        0.951416             0.9511      0.000000   \n",
       "172          2        13        0.951416             0.9511      0.000000   \n",
       "173          2        14        0.951416             0.9511      0.000000   \n",
       "174          2        15        0.951416             0.9511      0.000000   \n",
       "175          2        16        0.951416             0.9511      0.000000   \n",
       "176          2        17        0.951416             0.9511      0.000000   \n",
       "177          2        18        0.951416             0.9511      0.000000   \n",
       "178          2        19        0.951416             0.9511      0.000000   \n",
       "179          2        20        0.951416             0.9511      0.000000   \n",
       "180          1         1        0.951416             0.9511      0.000000   \n",
       "181          1         2        0.951416             0.9511      0.000000   \n",
       "182          1         3        0.951416             0.9511      0.000000   \n",
       "183          1         4        0.951416             0.9511      0.000000   \n",
       "184          1         5        0.951416             0.9511      0.000000   \n",
       "185          1         6        0.951416             0.9511      0.000000   \n",
       "186          1         7        0.951416             0.9511      0.000000   \n",
       "187          1         8        0.951416             0.9511      0.000000   \n",
       "188          1         9        0.951416             0.9511      0.000000   \n",
       "189          1        10        0.951416             0.9511      0.000000   \n",
       "190          1        11        0.951416             0.9511      0.000000   \n",
       "191          1        12        0.951416             0.9511      0.000000   \n",
       "192          1        13        0.951416             0.9511      0.000000   \n",
       "193          1        14        0.951416             0.9511      0.000000   \n",
       "194          1        15        0.951416             0.9511      0.000000   \n",
       "195          1        16        0.951416             0.9511      0.000000   \n",
       "196          1        17        0.951416             0.9511      0.000000   \n",
       "197          1        18        0.951416             0.9511      0.000000   \n",
       "198          1        19        0.951416             0.9511      0.000000   \n",
       "199          1        20        0.951416             0.9511      0.000000   \n",
       "\n",
       "     validate_recall  accuracy_difference  recall_difference  \n",
       "0           0.016667             0.000315           0.155995  \n",
       "20          0.016667             0.000315           0.127218  \n",
       "40          0.016667             0.000315           0.112830  \n",
       "1           0.000000             0.000315           0.086331  \n",
       "2           0.000000             0.000315           0.000000  \n",
       "3           0.000000             0.000315           0.000000  \n",
       "4           0.000000             0.000315           0.000000  \n",
       "5           0.000000             0.000315           0.000000  \n",
       "6           0.000000             0.000315           0.000000  \n",
       "7           0.000000             0.000315           0.000000  \n",
       "8           0.000000             0.000315           0.000000  \n",
       "9           0.000000             0.000315           0.000000  \n",
       "10          0.000000             0.000315           0.000000  \n",
       "11          0.000000             0.000315           0.000000  \n",
       "12          0.000000             0.000315           0.000000  \n",
       "13          0.000000             0.000315           0.000000  \n",
       "14          0.000000             0.000315           0.000000  \n",
       "15          0.000000             0.000315           0.000000  \n",
       "16          0.000000             0.000315           0.000000  \n",
       "17          0.000000             0.000315           0.000000  \n",
       "18          0.000000             0.000315           0.000000  \n",
       "19          0.000000             0.000315           0.000000  \n",
       "21          0.000000             0.000315           0.079137  \n",
       "22          0.000000             0.000315           0.000000  \n",
       "23          0.000000             0.000315           0.000000  \n",
       "24          0.000000             0.000315           0.000000  \n",
       "25          0.000000             0.000315           0.000000  \n",
       "26          0.000000             0.000315           0.000000  \n",
       "27          0.000000             0.000315           0.000000  \n",
       "28          0.000000             0.000315           0.000000  \n",
       "29          0.000000             0.000315           0.000000  \n",
       "30          0.000000             0.000315           0.000000  \n",
       "31          0.000000             0.000315           0.000000  \n",
       "32          0.000000             0.000315           0.000000  \n",
       "33          0.000000             0.000315           0.000000  \n",
       "34          0.000000             0.000315           0.000000  \n",
       "35          0.000000             0.000315           0.000000  \n",
       "36          0.000000             0.000315           0.000000  \n",
       "37          0.000000             0.000315           0.000000  \n",
       "38          0.000000             0.000315           0.000000  \n",
       "39          0.000000             0.000315           0.000000  \n",
       "41          0.000000             0.000315           0.064748  \n",
       "42          0.000000             0.000315           0.000000  \n",
       "43          0.000000             0.000315           0.000000  \n",
       "44          0.000000             0.000315           0.000000  \n",
       "45          0.000000             0.000315           0.000000  \n",
       "46          0.000000             0.000315           0.000000  \n",
       "47          0.000000             0.000315           0.000000  \n",
       "48          0.000000             0.000315           0.000000  \n",
       "49          0.000000             0.000315           0.000000  \n",
       "50          0.000000             0.000315           0.000000  \n",
       "51          0.000000             0.000315           0.000000  \n",
       "52          0.000000             0.000315           0.000000  \n",
       "53          0.000000             0.000315           0.000000  \n",
       "54          0.000000             0.000315           0.000000  \n",
       "55          0.000000             0.000315           0.000000  \n",
       "56          0.000000             0.000315           0.000000  \n",
       "57          0.000000             0.000315           0.000000  \n",
       "58          0.000000             0.000315           0.000000  \n",
       "59          0.000000             0.000315           0.000000  \n",
       "60          0.000000             0.000315           0.079137  \n",
       "61          0.000000             0.000315           0.014388  \n",
       "62          0.000000             0.000315           0.000000  \n",
       "63          0.000000             0.000315           0.000000  \n",
       "64          0.000000             0.000315           0.000000  \n",
       "65          0.000000             0.000315           0.000000  \n",
       "66          0.000000             0.000315           0.000000  \n",
       "67          0.000000             0.000315           0.000000  \n",
       "68          0.000000             0.000315           0.000000  \n",
       "69          0.000000             0.000315           0.000000  \n",
       "70          0.000000             0.000315           0.000000  \n",
       "71          0.000000             0.000315           0.000000  \n",
       "72          0.000000             0.000315           0.000000  \n",
       "73          0.000000             0.000315           0.000000  \n",
       "74          0.000000             0.000315           0.000000  \n",
       "75          0.000000             0.000315           0.000000  \n",
       "76          0.000000             0.000315           0.000000  \n",
       "77          0.000000             0.000315           0.000000  \n",
       "78          0.000000             0.000315           0.000000  \n",
       "79          0.000000             0.000315           0.000000  \n",
       "80          0.000000             0.000315           0.028777  \n",
       "81          0.000000             0.000315           0.000000  \n",
       "82          0.000000             0.000315           0.000000  \n",
       "83          0.000000             0.000315           0.000000  \n",
       "84          0.000000             0.000315           0.000000  \n",
       "85          0.000000             0.000315           0.000000  \n",
       "86          0.000000             0.000315           0.000000  \n",
       "87          0.000000             0.000315           0.000000  \n",
       "88          0.000000             0.000315           0.000000  \n",
       "89          0.000000             0.000315           0.000000  \n",
       "90          0.000000             0.000315           0.000000  \n",
       "91          0.000000             0.000315           0.000000  \n",
       "92          0.000000             0.000315           0.000000  \n",
       "93          0.000000             0.000315           0.000000  \n",
       "94          0.000000             0.000315           0.000000  \n",
       "95          0.000000             0.000315           0.000000  \n",
       "96          0.000000             0.000315           0.000000  \n",
       "97          0.000000             0.000315           0.000000  \n",
       "98          0.000000             0.000315           0.000000  \n",
       "99          0.000000             0.000315           0.000000  \n",
       "100         0.000000             0.000315           0.000000  \n",
       "101         0.000000             0.000315           0.000000  \n",
       "102         0.000000             0.000315           0.000000  \n",
       "103         0.000000             0.000315           0.000000  \n",
       "104         0.000000             0.000315           0.000000  \n",
       "105         0.000000             0.000315           0.000000  \n",
       "106         0.000000             0.000315           0.000000  \n",
       "107         0.000000             0.000315           0.000000  \n",
       "108         0.000000             0.000315           0.000000  \n",
       "109         0.000000             0.000315           0.000000  \n",
       "110         0.000000             0.000315           0.000000  \n",
       "111         0.000000             0.000315           0.000000  \n",
       "112         0.000000             0.000315           0.000000  \n",
       "113         0.000000             0.000315           0.000000  \n",
       "114         0.000000             0.000315           0.000000  \n",
       "115         0.000000             0.000315           0.000000  \n",
       "116         0.000000             0.000315           0.000000  \n",
       "117         0.000000             0.000315           0.000000  \n",
       "118         0.000000             0.000315           0.000000  \n",
       "119         0.000000             0.000315           0.000000  \n",
       "120         0.000000             0.000315           0.000000  \n",
       "121         0.000000             0.000315           0.000000  \n",
       "122         0.000000             0.000315           0.000000  \n",
       "123         0.000000             0.000315           0.000000  \n",
       "124         0.000000             0.000315           0.000000  \n",
       "125         0.000000             0.000315           0.000000  \n",
       "126         0.000000             0.000315           0.000000  \n",
       "127         0.000000             0.000315           0.000000  \n",
       "128         0.000000             0.000315           0.000000  \n",
       "129         0.000000             0.000315           0.000000  \n",
       "130         0.000000             0.000315           0.000000  \n",
       "131         0.000000             0.000315           0.000000  \n",
       "132         0.000000             0.000315           0.000000  \n",
       "133         0.000000             0.000315           0.000000  \n",
       "134         0.000000             0.000315           0.000000  \n",
       "135         0.000000             0.000315           0.000000  \n",
       "136         0.000000             0.000315           0.000000  \n",
       "137         0.000000             0.000315           0.000000  \n",
       "138         0.000000             0.000315           0.000000  \n",
       "139         0.000000             0.000315           0.000000  \n",
       "140         0.000000             0.000315           0.000000  \n",
       "141         0.000000             0.000315           0.000000  \n",
       "142         0.000000             0.000315           0.000000  \n",
       "143         0.000000             0.000315           0.000000  \n",
       "144         0.000000             0.000315           0.000000  \n",
       "145         0.000000             0.000315           0.000000  \n",
       "146         0.000000             0.000315           0.000000  \n",
       "147         0.000000             0.000315           0.000000  \n",
       "148         0.000000             0.000315           0.000000  \n",
       "149         0.000000             0.000315           0.000000  \n",
       "150         0.000000             0.000315           0.000000  \n",
       "151         0.000000             0.000315           0.000000  \n",
       "152         0.000000             0.000315           0.000000  \n",
       "153         0.000000             0.000315           0.000000  \n",
       "154         0.000000             0.000315           0.000000  \n",
       "155         0.000000             0.000315           0.000000  \n",
       "156         0.000000             0.000315           0.000000  \n",
       "157         0.000000             0.000315           0.000000  \n",
       "158         0.000000             0.000315           0.000000  \n",
       "159         0.000000             0.000315           0.000000  \n",
       "160         0.000000             0.000315           0.000000  \n",
       "161         0.000000             0.000315           0.000000  \n",
       "162         0.000000             0.000315           0.000000  \n",
       "163         0.000000             0.000315           0.000000  \n",
       "164         0.000000             0.000315           0.000000  \n",
       "165         0.000000             0.000315           0.000000  \n",
       "166         0.000000             0.000315           0.000000  \n",
       "167         0.000000             0.000315           0.000000  \n",
       "168         0.000000             0.000315           0.000000  \n",
       "169         0.000000             0.000315           0.000000  \n",
       "170         0.000000             0.000315           0.000000  \n",
       "171         0.000000             0.000315           0.000000  \n",
       "172         0.000000             0.000315           0.000000  \n",
       "173         0.000000             0.000315           0.000000  \n",
       "174         0.000000             0.000315           0.000000  \n",
       "175         0.000000             0.000315           0.000000  \n",
       "176         0.000000             0.000315           0.000000  \n",
       "177         0.000000             0.000315           0.000000  \n",
       "178         0.000000             0.000315           0.000000  \n",
       "179         0.000000             0.000315           0.000000  \n",
       "180         0.000000             0.000315           0.000000  \n",
       "181         0.000000             0.000315           0.000000  \n",
       "182         0.000000             0.000315           0.000000  \n",
       "183         0.000000             0.000315           0.000000  \n",
       "184         0.000000             0.000315           0.000000  \n",
       "185         0.000000             0.000315           0.000000  \n",
       "186         0.000000             0.000315           0.000000  \n",
       "187         0.000000             0.000315           0.000000  \n",
       "188         0.000000             0.000315           0.000000  \n",
       "189         0.000000             0.000315           0.000000  \n",
       "190         0.000000             0.000315           0.000000  \n",
       "191         0.000000             0.000315           0.000000  \n",
       "192         0.000000             0.000315           0.000000  \n",
       "193         0.000000             0.000315           0.000000  \n",
       "194         0.000000             0.000315           0.000000  \n",
       "195         0.000000             0.000315           0.000000  \n",
       "196         0.000000             0.000315           0.000000  \n",
       "197         0.000000             0.000315           0.000000  \n",
       "198         0.000000             0.000315           0.000000  \n",
       "199         0.000000             0.000315           0.000000  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "df2.sort_values(by=['validate_accuracy','validate_recall'],ascending= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "14e55944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max depth 10\n",
      "max depth 9\n",
      "max depth 8\n",
      "max depth 7\n",
      "max depth 6\n",
      "max depth 5\n",
      "max depth 4\n",
      "max depth 3\n",
      "max depth 2\n",
      "max depth 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>train_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.957707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.954561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0.951416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.951416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.951416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>0.951416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>0.951416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>0.951416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>0.951416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0.951416</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     max_depth  min_samples_leaf  train_accuracy\n",
       "0           10                 1        0.957707\n",
       "1           10                 2        0.954561\n",
       "2           10                 3        0.951416\n",
       "3           10                 4        0.951416\n",
       "4           10                 5        0.951416\n",
       "..         ...               ...             ...\n",
       "195          1                16        0.951416\n",
       "196          1                17        0.951416\n",
       "197          1                18        0.951416\n",
       "198          1                19        0.951416\n",
       "199          1                20        0.951416\n",
       "\n",
       "[200 rows x 3 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "metrics=[]\n",
    "for h in range(10,0,-1):\n",
    "   \n",
    "    print(f'max depth {h}')\n",
    "    \n",
    "    for i in range(1, 21):\n",
    "  \n",
    "    # Make the model\n",
    "        random_forest = RandomForestClassifier(max_depth=h, min_samples_leaf = i , random_state=123)\n",
    "    \n",
    "    # Fit the model (on train and only train)\n",
    "        random_forest.fit(X_train, y_train)\n",
    "        \n",
    "         # Use the model\n",
    "    # We'll evaluate the model's performance on train, first\n",
    "        in_sample_accuracy = random_forest.score(X_train, y_train)\n",
    "\n",
    "    # Use the model\n",
    "    # We'll evaluate the model's performance on train, first\n",
    "        y_pred = random_forest.predict(X_train)\n",
    "        \n",
    "        output = {\n",
    "        \"max_depth\": h,\n",
    "        \"min_samples_leaf\": i,\n",
    "        \"train_accuracy\": in_sample_accuracy,\n",
    "        #\"validate_accuracy\": y_pred\n",
    "    }\n",
    "\n",
    "        metrics.append(output)\n",
    "        \n",
    "        df5 = pd.DataFrame(metrics)\n",
    "df5\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "395d4356",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>train_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>0.951416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0.951416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0.951416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.951416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>0.951416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.954212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.954561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.956309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.957008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.957707</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     max_depth  min_samples_leaf  train_accuracy\n",
       "99           6                20        0.951416\n",
       "127          4                 8        0.951416\n",
       "128          4                 9        0.951416\n",
       "129          4                10        0.951416\n",
       "130          4                11        0.951416\n",
       "..         ...               ...             ...\n",
       "21           9                 2        0.954212\n",
       "1           10                 2        0.954561\n",
       "40           8                 1        0.956309\n",
       "20           9                 1        0.957008\n",
       "0           10                 1        0.957707\n",
       "\n",
       "[200 rows x 3 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(by='train_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3feb08",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "82e4c50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of KNN classifier on training set: 0.953\n"
     ]
    }
   ],
   "source": [
    "# weights = ['uniform', 'density']\n",
    "knn = KNeighborsClassifier(n_neighbors=5, weights='uniform')\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_train)\n",
    "print('Accuracy of KNN classifier on training set: {:.3f}'\n",
    "     .format(knn.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9bc4cf29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neighbor</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>validate_accuracy</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>validate_recall</th>\n",
       "      <th>accuracy_difference</th>\n",
       "      <th>recall_difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.931143</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.316547</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>-0.019957</td>\n",
       "      <td>0.233213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.954212</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.071942</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.003112</td>\n",
       "      <td>0.038609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.151079</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.084412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.952115</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.021583</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.001014</td>\n",
       "      <td>0.004916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.952814</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.035971</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.001713</td>\n",
       "      <td>0.002638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>-0.016667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.952115</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.014388</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.001014</td>\n",
       "      <td>-0.002278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>-0.016667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.951066</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.016667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    neighbor  train_accuracy  validate_accuracy  train_recall  \\\n",
       "0          1        0.931143             0.9511      0.316547   \n",
       "1          2        0.954212             0.9511      0.071942   \n",
       "2          3        0.951416             0.9511      0.151079   \n",
       "3          4        0.952115             0.9511      0.021583   \n",
       "4          5        0.952814             0.9511      0.035971   \n",
       "5          6        0.951416             0.9511      0.000000   \n",
       "6          7        0.952115             0.9511      0.014388   \n",
       "7          8        0.951416             0.9511      0.000000   \n",
       "8          9        0.951066             0.9511      0.000000   \n",
       "9         10        0.951416             0.9511      0.000000   \n",
       "10        11        0.951416             0.9511      0.000000   \n",
       "11        12        0.951416             0.9511      0.000000   \n",
       "12        13        0.951416             0.9511      0.000000   \n",
       "13        14        0.951416             0.9511      0.000000   \n",
       "14        15        0.951416             0.9511      0.000000   \n",
       "15        16        0.951416             0.9511      0.000000   \n",
       "16        17        0.951416             0.9511      0.000000   \n",
       "17        18        0.951416             0.9511      0.000000   \n",
       "18        19        0.951416             0.9511      0.000000   \n",
       "19        20        0.951416             0.9511      0.000000   \n",
       "\n",
       "    validate_recall  accuracy_difference  recall_difference  \n",
       "0          0.083333            -0.019957           0.233213  \n",
       "1          0.033333             0.003112           0.038609  \n",
       "2          0.066667             0.000315           0.084412  \n",
       "3          0.016667             0.001014           0.004916  \n",
       "4          0.033333             0.001713           0.002638  \n",
       "5          0.016667             0.000315          -0.016667  \n",
       "6          0.016667             0.001014          -0.002278  \n",
       "7          0.016667             0.000315          -0.016667  \n",
       "8          0.016667            -0.000034          -0.016667  \n",
       "9          0.000000             0.000315           0.000000  \n",
       "10         0.000000             0.000315           0.000000  \n",
       "11         0.000000             0.000315           0.000000  \n",
       "12         0.000000             0.000315           0.000000  \n",
       "13         0.000000             0.000315           0.000000  \n",
       "14         0.000000             0.000315           0.000000  \n",
       "15         0.000000             0.000315           0.000000  \n",
       "16         0.000000             0.000315           0.000000  \n",
       "17         0.000000             0.000315           0.000000  \n",
       "18         0.000000             0.000315           0.000000  \n",
       "19         0.000000             0.000315           0.000000  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics=[]\n",
    "for i in range(1,21):\n",
    "    # weights = ['uniform', 'density']\n",
    "    knn = KNeighborsClassifier(n_neighbors=i, weights='uniform')\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_train)\n",
    "    \n",
    "\n",
    "    in_sample_accuracy =knn.score(X_train, y_train)\n",
    "\n",
    "    out_sample_accuracy =knn.score(X_validate, y_validate)\n",
    "\n",
    "    # calculate recall train\n",
    "    y_pred = knn.predict(X_train)\n",
    "    in_sample_recall= recall_score(y_train, y_pred)  \n",
    "\n",
    "    # calculate recall validate\n",
    "    y_pred = knn.predict(X_validate)\n",
    "    out_of_sample_recall= recall_score(y_validate, y_pred)\n",
    "\n",
    "    output = {\n",
    "        \"neighbor\": i,\n",
    "        \"train_accuracy\": in_sample_accuracy,\n",
    "        \"validate_accuracy\": out_of_sample_accuracy,\n",
    "        'train_recall': in_sample_recall,\n",
    "        'validate_recall': out_of_sample_recall\n",
    "    }\n",
    "\n",
    "    metrics.append(output)\n",
    "\n",
    "df = pd.DataFrame(metrics)\n",
    "df[\"accuracy_difference\"] = df.train_accuracy - df.validate_accuracy\n",
    "df[\"recall_difference\"] = df.train_recall - df.validate_recall\n",
    "df\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "50bc2486",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neighbor</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>validate_accuracy</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>validate_recall</th>\n",
       "      <th>accuracy_difference</th>\n",
       "      <th>recall_difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.931143</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.316547</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>-0.019957</td>\n",
       "      <td>0.233213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.151079</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.084412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.954212</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.071942</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.003112</td>\n",
       "      <td>0.038609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.952814</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.035971</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.001713</td>\n",
       "      <td>0.002638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.952115</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.021583</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.001014</td>\n",
       "      <td>0.004916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>-0.016667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.952115</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.014388</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.001014</td>\n",
       "      <td>-0.002278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>-0.016667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.951066</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.016667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    neighbor  train_accuracy  validate_accuracy  train_recall  \\\n",
       "0          1        0.931143             0.9511      0.316547   \n",
       "2          3        0.951416             0.9511      0.151079   \n",
       "1          2        0.954212             0.9511      0.071942   \n",
       "4          5        0.952814             0.9511      0.035971   \n",
       "3          4        0.952115             0.9511      0.021583   \n",
       "5          6        0.951416             0.9511      0.000000   \n",
       "6          7        0.952115             0.9511      0.014388   \n",
       "7          8        0.951416             0.9511      0.000000   \n",
       "8          9        0.951066             0.9511      0.000000   \n",
       "9         10        0.951416             0.9511      0.000000   \n",
       "10        11        0.951416             0.9511      0.000000   \n",
       "11        12        0.951416             0.9511      0.000000   \n",
       "12        13        0.951416             0.9511      0.000000   \n",
       "13        14        0.951416             0.9511      0.000000   \n",
       "14        15        0.951416             0.9511      0.000000   \n",
       "15        16        0.951416             0.9511      0.000000   \n",
       "16        17        0.951416             0.9511      0.000000   \n",
       "17        18        0.951416             0.9511      0.000000   \n",
       "18        19        0.951416             0.9511      0.000000   \n",
       "19        20        0.951416             0.9511      0.000000   \n",
       "\n",
       "    validate_recall  accuracy_difference  recall_difference  \n",
       "0          0.083333            -0.019957           0.233213  \n",
       "2          0.066667             0.000315           0.084412  \n",
       "1          0.033333             0.003112           0.038609  \n",
       "4          0.033333             0.001713           0.002638  \n",
       "3          0.016667             0.001014           0.004916  \n",
       "5          0.016667             0.000315          -0.016667  \n",
       "6          0.016667             0.001014          -0.002278  \n",
       "7          0.016667             0.000315          -0.016667  \n",
       "8          0.016667            -0.000034          -0.016667  \n",
       "9          0.000000             0.000315           0.000000  \n",
       "10         0.000000             0.000315           0.000000  \n",
       "11         0.000000             0.000315           0.000000  \n",
       "12         0.000000             0.000315           0.000000  \n",
       "13         0.000000             0.000315           0.000000  \n",
       "14         0.000000             0.000315           0.000000  \n",
       "15         0.000000             0.000315           0.000000  \n",
       "16         0.000000             0.000315           0.000000  \n",
       "17         0.000000             0.000315           0.000000  \n",
       "18         0.000000             0.000315           0.000000  \n",
       "19         0.000000             0.000315           0.000000  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(by=['validate_recall','validate_recall'],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e64945aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neighbor</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>validate_accuracy</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>validate_recall</th>\n",
       "      <th>accuracy_difference</th>\n",
       "      <th>recall_difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.931143</td>\n",
       "      <td>0.93806</td>\n",
       "      <td>0.316547</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>-0.006917</td>\n",
       "      <td>0.233213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.956659</td>\n",
       "      <td>0.93806</td>\n",
       "      <td>0.122302</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.018598</td>\n",
       "      <td>0.072302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.954561</td>\n",
       "      <td>0.93806</td>\n",
       "      <td>0.187050</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.016501</td>\n",
       "      <td>0.103717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.957707</td>\n",
       "      <td>0.93806</td>\n",
       "      <td>0.158273</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.019647</td>\n",
       "      <td>0.108273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.957707</td>\n",
       "      <td>0.93806</td>\n",
       "      <td>0.158273</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.019647</td>\n",
       "      <td>0.108273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.957707</td>\n",
       "      <td>0.93806</td>\n",
       "      <td>0.158273</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.019647</td>\n",
       "      <td>0.108273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.957707</td>\n",
       "      <td>0.93806</td>\n",
       "      <td>0.158273</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.019647</td>\n",
       "      <td>0.124940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.957707</td>\n",
       "      <td>0.93806</td>\n",
       "      <td>0.158273</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.019647</td>\n",
       "      <td>0.124940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.957707</td>\n",
       "      <td>0.93806</td>\n",
       "      <td>0.158273</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.019647</td>\n",
       "      <td>0.124940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.957707</td>\n",
       "      <td>0.93806</td>\n",
       "      <td>0.158273</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.019647</td>\n",
       "      <td>0.141607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0.957707</td>\n",
       "      <td>0.93806</td>\n",
       "      <td>0.158273</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.019647</td>\n",
       "      <td>0.141607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0.957707</td>\n",
       "      <td>0.93806</td>\n",
       "      <td>0.158273</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.019647</td>\n",
       "      <td>0.141607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0.957707</td>\n",
       "      <td>0.93806</td>\n",
       "      <td>0.158273</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.019647</td>\n",
       "      <td>0.141607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0.957707</td>\n",
       "      <td>0.93806</td>\n",
       "      <td>0.158273</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.019647</td>\n",
       "      <td>0.141607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0.957707</td>\n",
       "      <td>0.93806</td>\n",
       "      <td>0.158273</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.019647</td>\n",
       "      <td>0.141607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>0.957707</td>\n",
       "      <td>0.93806</td>\n",
       "      <td>0.158273</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.019647</td>\n",
       "      <td>0.141607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>0.957707</td>\n",
       "      <td>0.93806</td>\n",
       "      <td>0.158273</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.019647</td>\n",
       "      <td>0.141607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>0.957707</td>\n",
       "      <td>0.93806</td>\n",
       "      <td>0.158273</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.019647</td>\n",
       "      <td>0.141607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>0.957707</td>\n",
       "      <td>0.93806</td>\n",
       "      <td>0.158273</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.019647</td>\n",
       "      <td>0.141607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>0.957707</td>\n",
       "      <td>0.93806</td>\n",
       "      <td>0.158273</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.019647</td>\n",
       "      <td>0.141607</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    neighbor  train_accuracy  validate_accuracy  train_recall  \\\n",
       "0          1        0.931143            0.93806      0.316547   \n",
       "1          2        0.956659            0.93806      0.122302   \n",
       "2          3        0.954561            0.93806      0.187050   \n",
       "3          4        0.957707            0.93806      0.158273   \n",
       "4          5        0.957707            0.93806      0.158273   \n",
       "5          6        0.957707            0.93806      0.158273   \n",
       "6          7        0.957707            0.93806      0.158273   \n",
       "7          8        0.957707            0.93806      0.158273   \n",
       "8          9        0.957707            0.93806      0.158273   \n",
       "9         10        0.957707            0.93806      0.158273   \n",
       "10        11        0.957707            0.93806      0.158273   \n",
       "11        12        0.957707            0.93806      0.158273   \n",
       "12        13        0.957707            0.93806      0.158273   \n",
       "13        14        0.957707            0.93806      0.158273   \n",
       "14        15        0.957707            0.93806      0.158273   \n",
       "15        16        0.957707            0.93806      0.158273   \n",
       "16        17        0.957707            0.93806      0.158273   \n",
       "17        18        0.957707            0.93806      0.158273   \n",
       "18        19        0.957707            0.93806      0.158273   \n",
       "19        20        0.957707            0.93806      0.158273   \n",
       "\n",
       "    validate_recall  accuracy_difference  recall_difference  \n",
       "0          0.083333            -0.006917           0.233213  \n",
       "1          0.050000             0.018598           0.072302  \n",
       "2          0.083333             0.016501           0.103717  \n",
       "3          0.050000             0.019647           0.108273  \n",
       "4          0.050000             0.019647           0.108273  \n",
       "5          0.050000             0.019647           0.108273  \n",
       "6          0.033333             0.019647           0.124940  \n",
       "7          0.033333             0.019647           0.124940  \n",
       "8          0.033333             0.019647           0.124940  \n",
       "9          0.016667             0.019647           0.141607  \n",
       "10         0.016667             0.019647           0.141607  \n",
       "11         0.016667             0.019647           0.141607  \n",
       "12         0.016667             0.019647           0.141607  \n",
       "13         0.016667             0.019647           0.141607  \n",
       "14         0.016667             0.019647           0.141607  \n",
       "15         0.016667             0.019647           0.141607  \n",
       "16         0.016667             0.019647           0.141607  \n",
       "17         0.016667             0.019647           0.141607  \n",
       "18         0.016667             0.019647           0.141607  \n",
       "19         0.016667             0.019647           0.141607  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics=[]\n",
    "for i in range(1,21):\n",
    "    # weights = ['uniform', 'density']\n",
    "    knn = KNeighborsClassifier(n_neighbors=i, weights='distance')\n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    in_sample_accuracy =knn.score(X_train, y_train)\n",
    "     \n",
    "    # calculate recall train\n",
    "    y_pred = knn.predict(X_train)\n",
    "    in_sample_recall= recall_score(y_train, y_pred)  \n",
    "\n",
    "    # calculate recall validate\n",
    "    y_pred = knn.predict(X_validate)\n",
    "    out_of_sample_recall= recall_score(y_validate, y_pred)\n",
    "\n",
    "    output = {\n",
    "        \"neighbor\": i,\n",
    "        \"train_accuracy\": in_sample_accuracy,\n",
    "        \"validate_accuracy\": out_of_sample_accuracy,\n",
    "        'train_recall': in_sample_recall,\n",
    "        'validate_recall': out_of_sample_recall\n",
    "    }\n",
    "\n",
    "    metrics.append(output)\n",
    "\n",
    "df = pd.DataFrame(metrics)\n",
    "df[\"accuracy_difference\"] = df.train_accuracy - df.validate_accuracy\n",
    "df[\"recall_difference\"] = df.train_recall - df.validate_recall\n",
    "df\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de01982",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8317cbfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>validate_accuracy</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>validate_recall</th>\n",
       "      <th>accuracy_difference</th>\n",
       "      <th>recall_difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.951416</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_accuracy  validate_accuracy  train_recall  validate_recall  \\\n",
       "0        0.951416             0.9511           0.0              0.0   \n",
       "\n",
       "   accuracy_difference  recall_difference  \n",
       "0             0.000315                0.0  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics=[]\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "logit = LogisticRegression(C=1, random_state=123, fit_intercept=True,solver ='saga' ,penalty= 'elasticnet'\n",
    "                          ,l1_ratio=.8)\n",
    "logit.fit(X_train, y_train)\n",
    "\n",
    "in_sample_accuracy =logit.score(X_train, y_train)\n",
    "out_sample_accuracy=logit.score(X_validate, y_validate)\n",
    "\n",
    "# calculate recall train\n",
    "y_pred = logit.predict(X_train)\n",
    "in_sample_recall= recall_score(y_train, y_pred)  \n",
    "\n",
    "# calculate recall validate\n",
    "y_pred = logit.predict(X_validate)\n",
    "out_of_sample_recall= recall_score(y_validate, y_pred)\n",
    "\n",
    "output = {\n",
    " \n",
    "    \"train_accuracy\": in_sample_accuracy,\n",
    "    \"validate_accuracy\": out_of_sample_accuracy,\n",
    "    'train_recall': in_sample_recall,\n",
    "    'validate_recall': out_of_sample_recall\n",
    "}\n",
    "\n",
    "metrics.append(output)\n",
    "\n",
    "df3 = pd.DataFrame(metrics)\n",
    "df3[\"accuracy_difference\"] = df3.train_accuracy - df3.validate_accuracy\n",
    "df3[\"recall_difference\"] = df3.train_recall - df3.validate_recall\n",
    "df3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb38518f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbb602e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d3e7a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebba348",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
